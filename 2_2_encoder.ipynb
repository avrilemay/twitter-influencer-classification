{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2_2_encoder.ipynb\n",
    "Fine-tuning Camemberta on multi-card user data.\n",
    "\n",
    "**Pipeline:**\n",
    "1. Train on user 'cards' (aggregated tweets)\n",
    "2. Use stratified k-fold split by user (to avoid leakage)\n",
    "3. Aggregate card predictions to get final user score\n",
    "\n"
   ],
   "id": "5c4752bc8373146f"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6eac4e7d198ea135",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765119734020,
     "user_tz": -60,
     "elapsed": 5570377,
     "user": {
      "displayName": "Avrile Floro",
      "userId": "06557886888439066437"
     }
    },
    "outputId": "9dffd818-36ea-4c10-b012-342ee293fa98"
   },
   "cell_type": "code",
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fine-tuning CamemBERT\n",
      "Hyperparameters -> LR: 2.32e-05, Batch: 48, WD: 0.019\n",
      "Output Dir: /content/drive/MyDrive/Colab Notebooks/code/intermediate\n",
      "\n",
      "1. Loading and cleaning data...\n",
      "  > Train Set: 61337 cards\n",
      "  > Test Set:  40894 cards\n",
      "\n",
      "2. Preparing User-Level Cross-Validation...\n",
      "\n",
      "3. Starting 5-Fold Cross-Validation...\n",
      "\n",
      "--- Fold 1/5 ---\n",
      "  Data: Train 49064 cards | Val 12273 cards\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at almanach/camembertav2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2046' max='2046' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2046/2046 15:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.320600</td>\n",
       "      <td>0.349619</td>\n",
       "      <td>0.856596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.265200</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.868247</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  > Fold Finished. Validation Card Accuracy: 0.8682\n",
      "\n",
      "--- Fold 2/5 ---\n",
      "  Data: Train 49073 cards | Val 12264 cards\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at almanach/camembertav2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2046' max='2046' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2046/2046 15:57, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.311500</td>\n",
       "      <td>0.333523</td>\n",
       "      <td>0.864726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271900</td>\n",
       "      <td>0.337372</td>\n",
       "      <td>0.871575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  > Fold Finished. Validation Card Accuracy: 0.8716\n",
      "\n",
      "--- Fold 3/5 ---\n",
      "  Data: Train 49071 cards | Val 12266 cards\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at almanach/camembertav2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2046' max='2046' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2046/2046 15:55, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.321600</td>\n",
       "      <td>0.356376</td>\n",
       "      <td>0.861406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.346454</td>\n",
       "      <td>0.868661</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  > Fold Finished. Validation Card Accuracy: 0.8686\n",
      "\n",
      "--- Fold 4/5 ---\n",
      "  Data: Train 49070 cards | Val 12267 cards\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at almanach/camembertav2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2046' max='2046' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2046/2046 15:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.314000</td>\n",
       "      <td>0.346675</td>\n",
       "      <td>0.863536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.275500</td>\n",
       "      <td>0.312141</td>\n",
       "      <td>0.876416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  > Fold Finished. Validation Card Accuracy: 0.8764\n",
      "\n",
      "--- Fold 5/5 ---\n",
      "  Data: Train 49070 cards | Val 12267 cards\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at almanach/camembertav2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2046' max='2046' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2046/2046 15:54, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.321900</td>\n",
       "      <td>0.317816</td>\n",
       "      <td>0.872177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.269200</td>\n",
       "      <td>0.328393</td>\n",
       "      <td>0.878291</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  > Fold Finished. Validation Card Accuracy: 0.8782\n",
      "\n",
      "4. Aggregating results (Card Level -> User Level)...\n",
      "\n",
      ">>> FINAL OOF ACCURACY (User-Level): 0.8749 <<<\n",
      "\n",
      "5. Saving CSV results...\n",
      "  [Saved] Training OOF predictions: /content/drive/MyDrive/Colab Notebooks/code/intermediate/oof_camembert.csv\n",
      "  [Saved] Test predictions: /content/drive/MyDrive/Colab Notebooks/code/intermediate/test_camembert.csv\n",
      "\n",
      "Script completed successfully.\n"
     ]
    }
   ],
   "execution_count": 4,
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "\n",
    "# configuration & reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# directory paths\n",
    "ROOT_DIR = \".\"\n",
    "# ROOT_DIR = \"/content/drive/MyDrive/Colab Notebooks/code\"  # for google colab\n",
    "OUT_DIR = os.path.join(ROOT_DIR, \"intermediate\")\n",
    "\n",
    "# create output dir if needed\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# inputs\n",
    "TRAIN_CSV = os.path.join(OUT_DIR, \"user_cards_train.csv\")\n",
    "TEST_CSV = os.path.join(OUT_DIR, \"user_cards_test.csv\")\n",
    "\n",
    "# outputs\n",
    "# oof = out-of-fold predictions (for training set evaluation)\n",
    "OUT_OOF = os.path.join(OUT_DIR, \"oof_camembert.csv\")\n",
    "OUT_TEST = os.path.join(OUT_DIR, \"test_camembert.csv\")\n",
    "\n",
    "# column mapping\n",
    "TEXT_COL = \"prompt\"       # input text\n",
    "LABEL_COL = \"user_label\"  # target (0 or 1)\n",
    "ID_COL = \"user_key\"       # user id for grouping\n",
    "\n",
    "# model config\n",
    "# camembertav2 is better for french text\n",
    "MODEL_NAME = \"almanach/camembertav2-base\"\n",
    "\n",
    "# max context size allowed by bert architecture\n",
    "MAX_LEN = 512\n",
    "\n",
    "# standard 5-fold cv (80% train / 20% val)\n",
    "N_FOLDS = 5\n",
    "\n",
    "# number of full passes over data\n",
    "EPOCHS = 2\n",
    "\n",
    "# optimized hyperparameters\n",
    "# learning rate: tuned low to avoid destabilizing pre-trained weights\n",
    "BEST_LR = 2.32e-05\n",
    "\n",
    "# batch sizes\n",
    "BEST_BS_T = 48  # fit as much as vram allows\n",
    "BEST_BS_E = 64  # larger for eval since no gradients stored\n",
    "\n",
    "# weight decay for regularization (prevents overfitting)\n",
    "BEST_WD = 0.019\n",
    "\n",
    "# label smoothing: prevents overconfidence by targeting 0.005/0.995 instead of 0/1\n",
    "LABEL_SMOOTHING = 0.005\n",
    "\n",
    "# linear decay is standard for transformer fine-tuning\n",
    "SCHEDULER_TYPE = \"linear\"\n",
    "\n",
    "\n",
    "# dataset class\n",
    "\n",
    "class CardDataset(Dataset):\n",
    "    \"\"\"\n",
    "    pytorch dataset wrapper\n",
    "    connects raw text list to dataloader\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, texts, labels=None, tokenizer=None, max_len=512):\n",
    "        self.texts = list(texts)\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        fetches i-th sample\n",
    "        tokenization done on-the-fly to save ram\n",
    "        \"\"\"\n",
    "        enc = self.tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,        # hard cut if text > 512\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        # squeeze removes batch dim (1, seq) -> (seq)\n",
    "        # dataloader expects single sample, batch dim added later\n",
    "        enc = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "\n",
    "        # add label if available (training mode)\n",
    "        if self.labels is not None:\n",
    "            enc[\"labels\"] = torch.tensor(int(self.labels[idx]), dtype=torch.long)\n",
    "\n",
    "        return enc\n",
    "\n",
    "\n",
    "# utility functions\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    simple accuracy metric for trainer callback\n",
    "    \"\"\"\n",
    "    logits, labels = eval_pred\n",
    "    # convert raw scores to class index (0 or 1)\n",
    "    predictions = logits.argmax(axis=1)\n",
    "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "def softmax(logits):\n",
    "    \"\"\"\n",
    "    converts logits to probabilities (0-1 range)\n",
    "    substract max for numerical stability to avoid overflow\n",
    "    \"\"\"\n",
    "    exp_logits = np.exp(logits - logits.max(axis=1, keepdims=True))\n",
    "    return exp_logits / exp_logits.sum(axis=1, keepdims=True)\n",
    "\n",
    "def predict_texts(model, tokenizer, texts, batch_size=32, max_length=512):\n",
    "    \"\"\"\n",
    "    efficient inference loop\n",
    "    runs in eval mode without gradients to save memory\n",
    "    \"\"\"\n",
    "    all_logits = []\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval() # disable dropout\n",
    "\n",
    "    with torch.no_grad(): # disable gradient graph construction\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i + batch_size]\n",
    "\n",
    "            # prep inputs\n",
    "            enc = tokenizer(\n",
    "                batch,\n",
    "                padding=True,       # dynamic padding to longest in batch\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            # move to device\n",
    "            enc = {k: v.to(device) for k, v in enc.items()}\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(**enc)\n",
    "            all_logits.append(outputs.logits.cpu().numpy())\n",
    "\n",
    "    return np.vstack(all_logits)\n",
    "\n",
    "\n",
    "\n",
    "# main training pipeline\n",
    "\n",
    "def main():\n",
    "    print(\"Fine-tuning CamemBERTav2\")\n",
    "    print(f\"Hyperparameters -> LR: {BEST_LR}, Batch: {BEST_BS_T}, WD: {BEST_WD}\")\n",
    "    print(f\"Output Dir: {OUT_DIR}\")\n",
    "\n",
    "    # 1. load and preprocess data\n",
    "    print(\"\\n1. Loading and cleaning data...\")\n",
    "    try:\n",
    "        train_cards = pd.read_csv(TRAIN_CSV)\n",
    "        test_cards = pd.read_csv(TEST_CSV)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"[error] Files not found. Check {OUT_DIR}\")\n",
    "        return\n",
    "\n",
    "    # basic cleanup: handle nans\n",
    "    train_cards[TEXT_COL] = train_cards[TEXT_COL].fillna(\"\").astype(str)\n",
    "\n",
    "    # remove garbage rows (empty text or no label)\n",
    "    mask_valid = (train_cards[TEXT_COL].str.len() > 0) & train_cards[LABEL_COL].notna()\n",
    "    train_cards = train_cards.loc[mask_valid].reset_index(drop=True)\n",
    "    train_cards[LABEL_COL] = train_cards[LABEL_COL].astype(int)\n",
    "\n",
    "    test_cards[TEXT_COL] = test_cards[TEXT_COL].fillna(\"\").astype(str)\n",
    "\n",
    "    print(f\"  > Train Set: {len(train_cards)} cards\")\n",
    "    print(f\"  > Test Set:  {len(test_cards)} cards\")\n",
    "\n",
    "    # 2. stratification\n",
    "    print(\"\\n2. Preparing User-Level Cross-Validation...\")\n",
    "\n",
    "    # must split by user, not card\n",
    "    # otherwise specific user syntax leaks into validation\n",
    "    user_labels = train_cards.drop_duplicates(ID_COL)[[ID_COL, LABEL_COL]].reset_index(drop=True)\n",
    "    user_ids = user_labels[ID_COL].values\n",
    "    labels_for_split = user_labels[LABEL_COL].values\n",
    "\n",
    "    # setup tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    fp16_available = torch.cuda.is_available() # use mixed precision if gpu\n",
    "\n",
    "    # 3. k-fold cross-validation loop\n",
    "    print(f\"\\n3. Starting {N_FOLDS}-Fold Cross-Validation...\")\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "    # storage for out-of-fold predictions\n",
    "    oof_proba_cards = np.zeros(len(train_cards), dtype=np.float32)\n",
    "\n",
    "    # list to accumulate test predictions from each fold model\n",
    "    test_proba_folds = []\n",
    "\n",
    "    # cache test texts\n",
    "    test_texts = test_cards[TEXT_COL].tolist()\n",
    "\n",
    "    for fold, (tr_user_idx, val_user_idx) in enumerate(\n",
    "        skf.split(np.zeros_like(labels_for_split), labels_for_split), 1\n",
    "    ):\n",
    "        print(f\"\\nFold {fold}/{N_FOLDS}\")\n",
    "\n",
    "        # map fold indices back to user ids\n",
    "        tr_users = set(user_ids[tr_user_idx])\n",
    "        val_users = set(user_ids[val_user_idx])\n",
    "\n",
    "        # split cards based on user mapping\n",
    "        tr_mask = train_cards[ID_COL].isin(tr_users)\n",
    "        val_mask = train_cards[ID_COL].isin(val_users)\n",
    "\n",
    "        tr_cards = train_cards.loc[tr_mask].reset_index(drop=True)\n",
    "        val_cards = train_cards.loc[val_mask].reset_index(drop=True)\n",
    "\n",
    "        # track original indices to fill oof array correctly later\n",
    "        val_original_indices = train_cards.loc[val_mask].index.values\n",
    "\n",
    "        print(f\"  Data: Train {len(tr_cards)} cards | Val {len(val_cards)} cards\")\n",
    "\n",
    "        # dataset objects\n",
    "        ds_tr = CardDataset(tr_cards[TEXT_COL].tolist(), tr_cards[LABEL_COL].values, tokenizer, MAX_LEN)\n",
    "        ds_val = CardDataset(val_cards[TEXT_COL].tolist(), val_cards[LABEL_COL].values, tokenizer, MAX_LEN)\n",
    "\n",
    "        # fresh model init for this fold\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "        # define training args\n",
    "        # note: no checkpoints saved to preserve disk space for final version\n",
    "        training_args = TrainingArguments(\n",
    "            output_dir= OUT_DIR,\n",
    "            # no checkpoint strategy\n",
    "            save_strategy=\"no\",           # dont save intermediate models\n",
    "            load_best_model_at_end=False, # take model at end of last epoch\n",
    "            learning_rate=BEST_LR,\n",
    "            per_device_train_batch_size=BEST_BS_T,\n",
    "            per_device_eval_batch_size=BEST_BS_E,\n",
    "            num_train_epochs=EPOCHS,\n",
    "            weight_decay=BEST_WD,\n",
    "            warmup_ratio=0.10,                  # warm up lr for first 10%\n",
    "            lr_scheduler_type=SCHEDULER_TYPE,\n",
    "            max_grad_norm=1.0,                  # clip gradients to prevent explosions\n",
    "            label_smoothing_factor=LABEL_SMOOTHING,\n",
    "            logging_steps=100,\n",
    "            eval_strategy=\"epoch\",              # check metrics every epoch\n",
    "            fp16=fp16_available,                # speed up on gpu\n",
    "            report_to=[],                       # no wandb/tensorboard logging\n",
    "            seed=SEED,\n",
    "        )\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=ds_tr,\n",
    "            eval_dataset=ds_val,\n",
    "            data_collator=collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        # train\n",
    "        trainer.train()\n",
    "\n",
    "        # manual move to device for inference loop\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model.to(device)\n",
    "\n",
    "        # validation prediction (oof)\n",
    "        # predict on validation chunk\n",
    "        val_logits = predict_texts(\n",
    "            model, tokenizer,\n",
    "            val_cards[TEXT_COL].tolist(),\n",
    "            batch_size=32,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "        # extract proba for class 1\n",
    "        val_proba = softmax(val_logits)[:, 1]\n",
    "\n",
    "        # store in global array\n",
    "        oof_proba_cards[val_original_indices] = val_proba\n",
    "\n",
    "        # simple accuracy check\n",
    "        card_acc = accuracy_score(val_cards[LABEL_COL].values, (val_proba >= 0.5).astype(int))\n",
    "        print(f\"  > Fold Finished. Validation Card Accuracy: {card_acc:.4f}\")\n",
    "\n",
    "        # test prediction\n",
    "        # predict on full test set with this fold's model\n",
    "        test_logits = predict_texts(\n",
    "            model, tokenizer,\n",
    "            test_texts,\n",
    "            batch_size=32,\n",
    "            max_length=MAX_LEN\n",
    "        )\n",
    "        test_proba_folds.append(softmax(test_logits)[:, 1])\n",
    "\n",
    "        # cleanup to prevent oom on colab\n",
    "        del trainer, model, ds_tr, ds_val\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # 4. aggregation (card -> user)\n",
    "    print(\"\\n4. Aggregating results (Card Level -> User Level)...\")\n",
    "\n",
    "    # a. test set: ensemble averaging across folds\n",
    "    test_proba_mean = np.mean(np.stack(test_proba_folds), axis=0)\n",
    "\n",
    "    # b. oof (train set): group cards by user\n",
    "    train_cards_oof = train_cards.copy()\n",
    "    train_cards_oof[\"oof_proba\"] = oof_proba_cards\n",
    "\n",
    "    # mean of probabilities per user\n",
    "    oof_user = train_cards_oof.groupby(ID_COL).agg({\n",
    "        \"oof_proba\": \"mean\",\n",
    "        LABEL_COL: \"first\" # label is constant for user\n",
    "    }).reset_index()\n",
    "\n",
    "    # calc final user-level accuracy\n",
    "    oof_preds = (oof_user[\"oof_proba\"] >= 0.5).astype(int)\n",
    "    oof_acc = accuracy_score(oof_user[LABEL_COL], oof_preds)\n",
    "    print(f\"\\n> Final OOF accuracy (user-level): {oof_acc:.4f} <<<\")\n",
    "\n",
    "    # c. format test submission\n",
    "    test_cards_proba = test_cards.copy()\n",
    "    test_cards_proba[\"proba\"] = test_proba_mean\n",
    "    test_user = test_cards_proba.groupby(ID_COL)[\"proba\"].mean().reset_index()\n",
    "    test_user.columns = [ID_COL, \"camembert_proba\"]\n",
    "\n",
    "    # 5. saving\n",
    "    print(\"\\n5. Saving CSV results...\")\n",
    "    oof_user.to_csv(OUT_OOF, index=False)\n",
    "    test_user.to_csv(OUT_TEST, index=False)\n",
    "    print(f\"  [saved] Training OOF predictions: {OUT_OOF}\")\n",
    "    print(f\"  [saved] Test predictions: {OUT_TEST}\")\n",
    "\n",
    "    print(\"\\nScript completed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "6eac4e7d198ea135"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```text\n",
    "Fine-tuning CamemBERTa\n",
    "Hyperparameters -> LR: 2.32e-05, Batch: 48, WD: 0.019\n",
    "Output Dir: /content/drive/MyDrive/Colab Notebooks/code/intermediate\n",
    "\n",
    "1. Loading and cleaning data...\n",
    "  > Train Set: 61337 cards\n",
    "  > Test Set:  40894 cards\n",
    "\n",
    "2. Preparing User-Level Cross-Validation...\n",
    "\n",
    "3. Starting 5-Fold Cross-Validation...\n",
    "\n",
    "--- Fold 1/5 ---\n",
    "  Data: Train 49064 cards | Val 12273 cards\n",
    "  > Fold Finished. Validation Card Accuracy: 0.8682\n",
    "\n",
    "--- Fold 2/5 ---\n",
    "  Data: Train 49073 cards | Val 12264 cards\n",
    "  > Fold Finished. Validation Card Accuracy: 0.8716\n",
    "\n",
    "--- Fold 3/5 ---\n",
    "  Data: Train 49071 cards | Val 12266 cards\n",
    "  > Fold Finished. Validation Card Accuracy: 0.8686\n",
    "\n",
    "--- Fold 4/5 ---\n",
    "  Data: Train 49070 cards | Val 12267 cards\n",
    "  > Fold Finished. Validation Card Accuracy: 0.8764\n",
    "\n",
    "--- Fold 5/5 ---\n",
    "  Data: Train 49070 cards | Val 12267 cards\n",
    "  > Fold Finished. Validation Card Accuracy: 0.8782\n",
    "\n",
    "4. Aggregating results (Card Level -> User Level)...\n",
    "\n",
    ">>> FINAL OOF ACCURACY (User-Level): 0.8749 <<<\n",
    "\n",
    "5. Saving CSV results...\n",
    "  [Saved] Training OOF predictions: /content/drive/MyDrive/Colab Notebooks/code/intermediate/oof_camembert.csv\n",
    "  [Saved] Test predictions: /content/drive/MyDrive/Colab Notebooks/code/intermediate/test_camembert.csv\n",
    "\n",
    "Script completed successfully.\n",
    "```\n"
   ],
   "id": "c2b232bb9da45e8d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "A100"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
