{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "1_lightgbm.ipynb\n",
    "\n",
    "This notebook implements a **user-level LightGBM pipeline** for the *Influencer vs Observer* task.\n",
    "\n",
    "Pipeline Overview:\n",
    "1. Load and normalize JSONL data\n",
    "2. Structural features from tweets (temporal, textual, user profile)\n",
    "3. Process object columns with frequency encoding and presence flags\n",
    "4. Aggregate tweet-level features to user-level statistics\n",
    "5. Train LightGBM classifier with stratified k-fold cross-validation\n",
    "6. Generate out-of-fold predictions and submission file\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 760220,
     "status": "ok",
     "timestamp": 1765062004828,
     "user": {
      "displayName": "Avrile Floro",
      "userId": "06557886888439066437"
     },
     "user_tz": -60
    },
    "id": "4t3ChDYLAvgM",
    "outputId": "2a1a698d-c309-411c-c194-d5837a61fa2a",
    "ExecuteTime": {
     "end_time": "2025-12-11T08:55:53.450982Z",
     "start_time": "2025-12-11T08:49:00.970657Z"
    }
   },
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from urllib.parse import urlparse\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# CONFIGURATION\n",
    "\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "ID_COL = \"ID\"\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# Directory paths\n",
    "ROOT_DIR = \".\"\n",
    "# ROOT_DIR = \"/content/drive/MyDrive/Colab Notebooks/code\" # for google colab\n",
    "OUT_DIR = os.path.join(ROOT_DIR, \"intermediate\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "TRAIN_JSONL = os.path.join(ROOT_DIR, \"train.jsonl\")\n",
    "TEST_JSONL = os.path.join(ROOT_DIR, \"kaggle_test.jsonl\")\n",
    "\n",
    "# Twitter datetime format\n",
    "TWITTER_DT_FORMAT = \"%a %b %d %H:%M:%S %z %Y\"\n",
    "\n",
    "# Columns to exclude from feature engineering\n",
    "TEXT_COLS_EXCLUDE = [\"text\", \"extended_tweet.full_text\", \"full_text\"]\n",
    "\n",
    "# LightGBM hyperparameters\n",
    "LGBM_PARAMS = {\n",
    "    'n_estimators': 1500,        # number of trees\n",
    "    'learning_rate': 0.03,      # learning rate\n",
    "    'num_leaves': 31,           # tree complexity\n",
    "    'max_depth': 7,            # max tree depth\n",
    "    'min_child_samples': 20,   # minimum samples per leaf\n",
    "    'subsample': 0.8,          # row sampling\n",
    "    'colsample_bytree': 0.8,   # feature sampling\n",
    "    'reg_alpha': 0.1,          # l1 regularization\n",
    "    'reg_lambda': 1.0,         # l2 regularization\n",
    "    'random_state': SEED,      # reproducibility\n",
    "    'n_jobs': -1,              # use all cores\n",
    "    'verbose': -1              # no logs\n",
    "}\n",
    "\n",
    "\n",
    "# UTILITY FUNCTIONS\n",
    "\n",
    "def safe_upper_ratio(text):\n",
    "    \"\"\"Calculate the ratio of uppercase characters in text.\"\"\"\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return 0.0\n",
    "    return sum(1 for c in text if c.isupper()) / max(1, len(text))\n",
    "\n",
    "\n",
    "def is_hex_color(x):\n",
    "    \"\"\"Check if string is a valid 6-digit hex color code.\"\"\"\n",
    "    return isinstance(x, str) and re.fullmatch(r\"[0-9A-Fa-f]{6}\", x) is not None\n",
    "\n",
    "\n",
    "def hex_to_rgb(h):\n",
    "    \"\"\"Convert hex color code to RGB tuple.\"\"\"\n",
    "    if is_hex_color(h):\n",
    "        return tuple(int(h[i:i+2], 16) for i in (0, 2, 4))\n",
    "    return (0, 0, 0)\n",
    "\n",
    "\n",
    "def extract_domain(x):\n",
    "    \"\"\"Extract domain name from URL string.\"\"\"\n",
    "    if not isinstance(x, str) or len(x) < 5:\n",
    "        return \"none\"\n",
    "    try:\n",
    "        return urlparse(x).netloc or \"none\"\n",
    "    except:\n",
    "        return \"none\"\n",
    "\n",
    "\n",
    "def parse_list_like(x):\n",
    "    \"\"\"Parse list-like strings and handle various input types.\"\"\"\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    if pd.isna(x):\n",
    "        return []\n",
    "    if isinstance(x, str):\n",
    "        s = x.strip()\n",
    "        if s in (\"\", \"[]\"):\n",
    "            return []\n",
    "        try:\n",
    "            val = ast.literal_eval(s)\n",
    "            return val if isinstance(val, list) else []\n",
    "        except:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def detect_bool_like_series(s, max_unique=4):\n",
    "    \"\"\"\n",
    "    Detect if a pandas Series contains boolean-like values.\n",
    "    Returns True if values can be interpreted as boolean (True/False, 0/1).\n",
    "    \"\"\"\n",
    "    sample = s.dropna()\n",
    "    if sample.apply(lambda v: isinstance(v, (list, dict, set))).any():\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        vals = pd.unique(sample)\n",
    "    except TypeError:\n",
    "        return False\n",
    "\n",
    "    if len(vals) == 0 or len(vals) > max_unique:\n",
    "        return False\n",
    "\n",
    "    for v in vals:\n",
    "        if isinstance(v, bool):\n",
    "            continue\n",
    "        elif isinstance(v, (int, float)) and v in (0, 1):\n",
    "            continue\n",
    "        elif isinstance(v, str) and v.strip().lower() in (\"true\", \"false\", \"0\", \"1\"):\n",
    "            continue\n",
    "        else:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def map_bool_like_to_float(s):\n",
    "    \"\"\"Convert boolean-like values to float (1.0 or 0.0).\"\"\"\n",
    "    def _map(v):\n",
    "        if isinstance(v, bool):\n",
    "            return 1.0 if v else 0.0\n",
    "        if isinstance(v, (int, float)) and v in (0, 1):\n",
    "            return float(v)\n",
    "        if isinstance(v, str):\n",
    "            vs = v.strip().lower()\n",
    "            if vs in (\"true\", \"1\"):\n",
    "                return 1.0\n",
    "            if vs in (\"false\", \"0\"):\n",
    "                return 0.0\n",
    "        return np.nan\n",
    "    return s.apply(_map).astype(\"float32\")\n",
    "\n",
    "\n",
    "def is_mostly_listlike(s, threshold=0.3):\n",
    "    \"\"\"Check if a Series contains mostly list-like values. (minimum 30%)\"\"\"\n",
    "    sample = s.dropna().head(200)\n",
    "    if len(sample) == 0:\n",
    "        return False\n",
    "    return sample.apply(lambda v: isinstance(v, (list, dict, set))).mean() > threshold\n",
    "\n",
    "\n",
    "def balanced_sample_weight(y):\n",
    "    \"\"\"\n",
    "    Compute balanced class weights for training.\n",
    "    Helps address class imbalance by upweighting minority class.\n",
    "    \"\"\"\n",
    "    classes, counts = np.unique(y, return_counts=True)\n",
    "    weights = {c: len(y) / (len(classes) * n) for c, n in zip(classes, counts)}\n",
    "    return np.array([weights[v] for v in y], dtype=np.float32)\n",
    "\n",
    "\n",
    "# STEP 1: DATA LOADING\n",
    "\n",
    "print(\"Step 1: Loading JSONL data\")\n",
    "\n",
    "# Load and normalize nested JSON structures\n",
    "train_df = json_normalize(pd.read_json(TRAIN_JSONL, lines=True).to_dict(orient=\"records\"))\n",
    "test_df = json_normalize(pd.read_json(TEST_JSONL, lines=True).to_dict(orient=\"records\"))\n",
    "\n",
    "# Set ID column\n",
    "train_df[ID_COL] = train_df[\"challenge_id\"]\n",
    "test_df[ID_COL] = test_df[\"challenge_id\"]\n",
    "\n",
    "print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n",
    "\n",
    "\n",
    "# STEP 2: FEATURE ENGINEERING\n",
    "print(\"\\nStep 2: Engineering structural features\")\n",
    "\n",
    "# Temporal Features\n",
    "print(\"  - Temporal features (hour, weekday, month, year)\")\n",
    "for df_ in (train_df, test_df):\n",
    "    if \"created_at\" in df_.columns:\n",
    "        dt = pd.to_datetime(df_[\"created_at\"], format=TWITTER_DT_FORMAT, errors=\"coerce\")\n",
    "        df_[\"tweet_hour\"] = dt.dt.hour\n",
    "        df_[\"tweet_weekday\"] = dt.dt.weekday\n",
    "        df_[\"tweet_dayofmonth\"] = dt.dt.day\n",
    "        df_[\"tweet_month\"] = dt.dt.month\n",
    "        df_[\"tweet_year\"] = dt.dt.year\n",
    "        df_[\"is_weekend\"] = df_[\"tweet_weekday\"].isin([5, 6]).astype(\"float32\")\n",
    "\n",
    "# Account Age\n",
    "print(\"  - Account age features\")\n",
    "if \"user.created_at\" in train_df.columns:\n",
    "    for df_ in (train_df, test_df):\n",
    "        dt_user = pd.to_datetime(df_[\"user.created_at\"], format=TWITTER_DT_FORMAT, errors=\"coerce\")\n",
    "        dt_tweet = pd.to_datetime(df_[\"created_at\"], format=TWITTER_DT_FORMAT, errors=\"coerce\")\n",
    "        age_days = (dt_tweet - dt_user).dt.days.astype(\"float32\").clip(lower=0)\n",
    "        df_[\"user_account_age_days\"] = age_days\n",
    "        df_[\"log_user_account_age_days\"] = np.log1p(age_days)\n",
    "\n",
    "\n",
    "# Text Features\n",
    "print(\"  - Text-based features (length, hashtags, mentions)\")\n",
    "for df_ in (train_df, test_df):\n",
    "    if \"text\" in df_.columns:\n",
    "        txt = df_[\"text\"].fillna(\"\").astype(str)\n",
    "        df_[\"text_length\"] = txt.str.len().astype(\"float32\")\n",
    "        df_[\"text_n_hashtags\"] = txt.str.count(\"#\").astype(\"float32\")\n",
    "        df_[\"text_n_mentions\"] = txt.str.count(\"@\").astype(\"float32\")\n",
    "        df_[\"text_upper_ratio\"] = txt.apply(safe_upper_ratio).astype(\"float32\")\n",
    "\n",
    "\n",
    "# User Description Features\n",
    "print(\"  - User description features\")\n",
    "for df_ in (train_df, test_df):\n",
    "    if \"user.description\" in df_.columns:\n",
    "        desc = df_[\"user.description\"].fillna(\"\").astype(str)\n",
    "        df_[\"user_desc_len\"] = desc.str.len().astype(\"float32\")\n",
    "        df_[\"user_desc_has_url\"] = desc.str.contains(\"http\", regex=False).astype(\"float32\")\n",
    "\n",
    "\n",
    "# Presence flags\n",
    "print(\"  - Presence flags (location, banner, profile URL)\")\n",
    "for df_ in (train_df, test_df):\n",
    "    presence_cols = [\n",
    "        (\"user.location\", \"has_location\"),\n",
    "        (\"user.profile_banner_url\", \"has_banner\"),\n",
    "            (\"user.url\", \"has_profile_url\")\n",
    "    ]\n",
    "    for col, feat in presence_cols:\n",
    "        if col in df_.columns:\n",
    "            df_[feat] = df_[col].notna().astype(\"float32\")\n",
    "\n",
    "\n",
    "# Entity counts\n",
    "print(\"  - Entity counts (hashtags, mentions, URLs, symbols)\")\n",
    "ENTITIES_LIST_COLS = [\n",
    "    \"entities.hashtags\", \"entities.user_mentions\", \"entities.urls\", \"entities.symbols\",\n",
    "    \"extended_tweet.entities.hashtags\", \"extended_tweet.entities.user_mentions\",\n",
    "    \"extended_tweet.entities.urls\", \"extended_tweet.entities.symbols\",\n",
    "    \"quoted_status.entities.hashtags\", \"quoted_status.entities.user_mentions\",\n",
    "    \"quoted_status.entities.urls\", \"quoted_status.entities.symbols\",\n",
    "]\n",
    "for df_ in (train_df, test_df):\n",
    "    for col in ENTITIES_LIST_COLS:\n",
    "        if col in df_.columns:\n",
    "            df_[f\"{col}_n_items\"] = df_[col].apply(\n",
    "                lambda x: len(x) if isinstance(x, list) else 0\n",
    "            ).astype(\"float32\")\n",
    "\n",
    "    if \"entities.media\" in df_.columns:\n",
    "        df_[\"has_media\"] = df_[\"entities.media\"].notna().astype(\"float32\")\n",
    "\n",
    "\n",
    "# Profile color features (RGB)\n",
    "print(\"  - Profile color RGB components\")\n",
    "COLOR_COLS = [\n",
    "    \"user.profile_link_color\",\n",
    "    \"user.profile_background_color\",\n",
    "    \"user.profile_sidebar_fill_color\",\n",
    "    \"user.profile_sidebar_border_color\",\n",
    "    \"user.profile_text_color\"\n",
    "]\n",
    "for df_ in (train_df, test_df):\n",
    "    for col in COLOR_COLS:\n",
    "        if col in df_.columns:\n",
    "            rgb = df_[col].apply(lambda x: pd.Series(hex_to_rgb(x)))\n",
    "            df_[f\"{col}_r\"] = rgb[0].astype(\"float32\")\n",
    "            df_[f\"{col}_g\"] = rgb[1].astype(\"float32\")\n",
    "            df_[f\"{col}_b\"] = rgb[2].astype(\"float32\")\n",
    "\n",
    "\n",
    "# Source frequency encoding (Iphone, Android, Web, etc.)\n",
    "print(\"  - Source frequency encoding\")\n",
    "if \"source\" in train_df.columns:\n",
    "    source_freq = train_df[\"source\"].value_counts(normalize=True)\n",
    "    train_df[\"source_freq\"] = train_df[\"source\"].map(source_freq).fillna(0).astype(\"float32\")\n",
    "    test_df[\"source_freq\"] = test_df[\"source\"].map(source_freq).fillna(0).astype(\"float32\")\n",
    "\n",
    "\n",
    "# User URL domain features (encode domain frequency)\n",
    "print(\"  - User URL domain frequency\")\n",
    "if \"user.url\" in train_df.columns:\n",
    "    train_df[\"user_url_domain\"] = train_df[\"user.url\"].apply(extract_domain)\n",
    "    test_df[\"user_url_domain\"] = test_df[\"user.url\"].apply(extract_domain)\n",
    "    domain_freq = train_df[\"user_url_domain\"].value_counts(normalize=True)\n",
    "    train_df[\"user_url_domain_freq\"] = train_df[\"user_url_domain\"].map(domain_freq).fillna(0).astype(\"float32\")\n",
    "    test_df[\"user_url_domain_freq\"] = test_df[\"user_url_domain\"].map(domain_freq).fillna(0).astype(\"float32\")\n",
    "\n",
    "\n",
    "# Withheld countries (is present? in how many countries?)\n",
    "print(\"  - Withheld countries features\")\n",
    "withheld_cols = [\"withheld_in_countries\", \"quoted_status.withheld_in_countries\"]\n",
    "for col in withheld_cols:\n",
    "    for df_ in (train_df, test_df):\n",
    "        if col in df_.columns:\n",
    "            parsed = df_[col].apply(parse_list_like)\n",
    "            df_[f\"{col}_has_withheld\"] = parsed.apply(\n",
    "                lambda x: 1.0 if len(x) > 0 else 0.0\n",
    "            ).astype(\"float32\")\n",
    "            df_[f\"{col}_n_withheld_countries\"] = parsed.apply(\n",
    "                lambda x: float(len(x))\n",
    "            ).astype(\"float32\")\n",
    "\n",
    "\n",
    "# Coordinate features (is coordinates present?)\n",
    "print(\"  - Coordinate presence flags\")\n",
    "coord_cols = [\n",
    "    \"coordinates.coordinates\",\n",
    "    \"geo.coordinates\",\n",
    "    \"quoted_status.coordinates.coordinates\",\n",
    "    \"quoted_status.geo.coordinates\"\n",
    "]\n",
    "for col in coord_cols:\n",
    "    for df_ in (train_df, test_df):\n",
    "        if col in df_.columns:\n",
    "            parsed = df_[col].apply(parse_list_like)\n",
    "            df_[f\"{col}_has_coords\"] = parsed.apply(\n",
    "                lambda x: 1.0 if len(x) >= 2 else 0.0\n",
    "            ).astype(\"float32\")\n",
    "\n",
    "\n",
    "# Quoted status features (how old is the quoted tweet/user?)\n",
    "print(\"  - Quoted status temporal features\")\n",
    "for df_ in (train_df, test_df):\n",
    "    dt_main = None\n",
    "    if \"created_at\" in df_.columns:\n",
    "        dt_main = pd.to_datetime(df_[\"created_at\"], format=TWITTER_DT_FORMAT, errors=\"coerce\")\n",
    "\n",
    "    # Time lag between main tweet and quoted tweet\n",
    "    if \"quoted_status.created_at\" in df_.columns and dt_main is not None:\n",
    "        dt_quoted = pd.to_datetime(df_[\"quoted_status.created_at\"], format=TWITTER_DT_FORMAT, errors=\"coerce\")\n",
    "        lag_days = (dt_main - dt_quoted).dt.days.astype(\"float32\").clip(lower=0)\n",
    "        df_[\"quoted_tweet_lag_days\"] = lag_days\n",
    "        df_[\"quoted_tweet_is_recent_7d\"] = (lag_days <= 7).astype(\"float32\")\n",
    "\n",
    "    # Quoted user account age\n",
    "    if \"quoted_status.user.created_at\" in df_.columns and dt_main is not None:\n",
    "        dt_q_user = pd.to_datetime(df_[\"quoted_status.user.created_at\"], format=TWITTER_DT_FORMAT, errors=\"coerce\")\n",
    "        age_q = (dt_main - dt_q_user).dt.days.astype(\"float32\").clip(lower=0)\n",
    "        df_[\"quoted_user_account_age_days\"] = age_q\n",
    "        df_[\"log_quoted_user_account_age_days\"] = np.log1p(age_q)\n",
    "\n",
    "\n",
    "# Quoted status numeric features\n",
    "print(\"  - Quoted status numeric features and log transforms\")\n",
    "quoted_numeric_cols = [\n",
    "    \"quoted_status.user.favourites_count\",\n",
    "    \"quoted_status.user.listed_count\",\n",
    "    \"quoted_status.user.friends_count\",\n",
    "    \"quoted_status.user.followers_count\",\n",
    "    \"quoted_status.user.statuses_count\",\n",
    "    \"quoted_status.retweet_count\",\n",
    "    \"quoted_status.favorite_count\",\n",
    "    \"quoted_status.quote_count\",\n",
    "    \"quoted_status.reply_count\"\n",
    "]\n",
    "for col in quoted_numeric_cols:\n",
    "    for df_ in (train_df, test_df):\n",
    "        if col in df_.columns:\n",
    "            df_[col] = pd.to_numeric(df_[col], errors=\"coerce\").astype(\"float32\")\n",
    "            df_[f\"{col}_log1p\"] = np.log1p(df_[col]).astype(\"float32\")\n",
    "\n",
    "\n",
    "# Global mapping from quoted users to main users (train + test)\n",
    "print(\"  - Global mapping quoted user friends/followers to main users via created_at\")\n",
    "\n",
    "# required columns to build the global quoted-user mapping\n",
    "required_cols = [\n",
    "    \"user.created_at\",\n",
    "    \"quoted_status.user.created_at\",\n",
    "    \"quoted_status.user.friends_count\",\n",
    "    \"quoted_status.user.followers_count\",\n",
    "]\n",
    "\n",
    "# only if all required columns are available\n",
    "# in both train and test datasets (robust to schema variations):\n",
    "if all(c in train_df.columns for c in required_cols) and all(c in test_df.columns for c in required_cols):\n",
    "\n",
    "    # extract quoted-user statistics from the training set\n",
    "    tmp_train = train_df[[\n",
    "        \"quoted_status.user.created_at\",\n",
    "        \"quoted_status.user.friends_count\",\n",
    "        \"quoted_status.user.followers_count\"\n",
    "    ]]\n",
    "\n",
    "    # extract quoted-user statistics from the test set\n",
    "    tmp_test = test_df[[\n",
    "        \"quoted_status.user.created_at\",\n",
    "        \"quoted_status.user.friends_count\",\n",
    "        \"quoted_status.user.followers_count\"\n",
    "    ]]\n",
    "\n",
    "    # combine train and test to build a global mapping\n",
    "    combined_tmp = pd.concat([tmp_train, tmp_test], axis=0, ignore_index=True)\n",
    "\n",
    "    # keep only rows where the quoted user's creation time is known\n",
    "    # (proxy identifier for the user)\n",
    "    combined_tmp = combined_tmp.dropna(subset=[\"quoted_status.user.created_at\"])\n",
    "\n",
    "    if not combined_tmp.empty:\n",
    "        # numeric conversion of friends and followers counters\n",
    "        combined_tmp[\"quoted_status.user.friends_count\"] = pd.to_numeric(\n",
    "            combined_tmp[\"quoted_status.user.friends_count\"], errors=\"coerce\"\n",
    "        )\n",
    "        combined_tmp[\"quoted_status.user.followers_count\"] = pd.to_numeric(\n",
    "            combined_tmp[\"quoted_status.user.followers_count\"], errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        # aggregate quoted-user statistics by their account creation timestamp\n",
    "        # the mean is used to stabilize potentially noisy observations\n",
    "        agg_global = combined_tmp.groupby(\"quoted_status.user.created_at\").agg({\n",
    "            \"quoted_status.user.friends_count\": \"mean\",\n",
    "            \"quoted_status.user.followers_count\": \"mean\",\n",
    "        }).rename(columns={\n",
    "            \"quoted_status.user.friends_count\": \"mapped_user_friends_count\",\n",
    "            \"quoted_status.user.followers_count\": \"mapped_user_followers_count\",\n",
    "        })\n",
    "\n",
    "        # map aggregated quoted-user statistics back to the main users\n",
    "        # using user.created_at as a proxy key shared across interactions\n",
    "        for df_ in (train_df, test_df):\n",
    "            df_[\"user_friends_from_quoted\"] = df_[\"user.created_at\"].map(\n",
    "                agg_global[\"mapped_user_friends_count\"]\n",
    "            ).astype(\"float32\")\n",
    "            df_[\"user_followers_from_quoted\"] = df_[\"user.created_at\"].map(\n",
    "                agg_global[\"mapped_user_followers_count\"]\n",
    "            ).astype(\"float32\")\n",
    "\n",
    "    else:\n",
    "        # degenerate case: no valid quoted-user timestamps available\n",
    "        for df_ in (train_df, test_df):\n",
    "            df_[\"user_friends_from_quoted\"] = np.nan\n",
    "            df_[\"user_followers_from_quoted\"] = np.nan\n",
    "\n",
    "else:\n",
    "    # if the required quoted-user columns are absent\n",
    "    for df_ in (train_df, test_df):\n",
    "        df_[\"user_friends_from_quoted\"] = np.nan\n",
    "        df_[\"user_followers_from_quoted\"] = np.nan\n",
    "\n",
    "\n",
    "# STEP 3: GENERIC OBJECT COLUMN PROCESSING\n",
    "print(\"\\nStep 3: Processing generic object columns\")\n",
    "\n",
    "# Align columns between train and test\n",
    "all_cols = sorted(set(train_df.columns) | set(test_df.columns))\n",
    "for col in all_cols:\n",
    "    if col not in train_df.columns:\n",
    "        train_df[col] = np.nan\n",
    "    if col not in test_df.columns:\n",
    "        test_df[col] = np.nan\n",
    "\n",
    "# Process all object-type columns\n",
    "obj_cols = [c for c in train_df.columns if train_df[c].dtype == \"O\"]\n",
    "print(f\"  Processing {len(obj_cols)} object columns\")\n",
    "\n",
    "for col in obj_cols:\n",
    "    # Skip label, ID, and text columns\n",
    "    if col in [LABEL_COL, ID_COL] + TEXT_COLS_EXCLUDE:\n",
    "        continue\n",
    "\n",
    "    s_train = train_df[col]\n",
    "    s_test = test_df[col]\n",
    "    s_all = pd.concat([s_train, s_test], axis=0)\n",
    "\n",
    "    # Adds a presence flag (is the value missing or not?)\n",
    "    train_df[f\"{col}_is_present\"] = s_train.notna().astype(\"float32\")\n",
    "    test_df[f\"{col}_is_present\"] = s_test.notna().astype(\"float32\")\n",
    "\n",
    "    # Adds the string length as a numeric feature\n",
    "    train_df[f\"{col}_str_len\"] = s_train.fillna(\"\").astype(str).str.len().astype(\"float32\")\n",
    "    test_df[f\"{col}_str_len\"] = s_test.fillna(\"\").astype(str).str.len().astype(\"float32\")\n",
    "\n",
    "    # Skip list-like columns for frequency encoding\n",
    "    if is_mostly_listlike(s_all):\n",
    "        continue\n",
    "\n",
    "    # detects and encodes boolean-like columns\n",
    "    if detect_bool_like_series(s_train):\n",
    "        train_df[f\"{col}_bool\"] = map_bool_like_to_float(s_train)\n",
    "        test_df[f\"{col}_bool\"] = map_bool_like_to_float(s_test)\n",
    "\n",
    "    # applies frequency encoding to low-cardinality categorical features\n",
    "    try:\n",
    "        n_unique = s_all.nunique(dropna=True)\n",
    "    except TypeError:\n",
    "        continue\n",
    "\n",
    "    if 1 < n_unique <= 20:\n",
    "        freq = s_all.value_counts(normalize=True)\n",
    "        train_df[f\"{col}_freq\"] = s_train.map(freq).fillna(0).astype(\"float32\")\n",
    "        test_df[f\"{col}_freq\"] = s_test.map(freq).fillna(0).astype(\"float32\")\n",
    "\n",
    "\n",
    "# List-like Item Counts\n",
    "# For columns containing list-like values, we extract the number of items as a numeric feature\n",
    "print(\"  - Counting items in list-like columns\")\n",
    "for col in obj_cols:\n",
    "    if col in [LABEL_COL, ID_COL] + TEXT_COLS_EXCLUDE + ENTITIES_LIST_COLS:\n",
    "        continue\n",
    "\n",
    "    s_all = pd.concat([train_df[col], test_df[col]], axis=0)\n",
    "    if is_mostly_listlike(s_all):\n",
    "        train_df[f\"{col}_n_items\"] = train_df[col].apply(parse_list_like).apply(\n",
    "            lambda x: float(len(x))\n",
    "        ).astype(\"float32\")\n",
    "        test_df[f\"{col}_n_items\"] = test_df[col].apply(parse_list_like).apply(\n",
    "            lambda x: float(len(x))\n",
    "        ).astype(\"float32\")\n",
    "\n",
    "\n",
    "# For selected categorical columns, we apply frequency encoding based on train + test values\n",
    "print(\"  - Additional frequency encodings for categorical columns\")\n",
    "categorical_cols = [\n",
    "    \"lang\", \"user.lang\", \"filter_level\", \"user.translator_type\",\n",
    "    \"place.country\", \"place.country_code\", \"place.full_name\",\n",
    "    \"place.name\", \"place.type\", \"user.time_zone\", \"in_reply_to_screen_name\"\n",
    "]\n",
    "for col in categorical_cols:\n",
    "    if col not in train_df.columns:\n",
    "        continue\n",
    "    s_train = train_df[col].astype(str)\n",
    "    s_test = test_df[col].astype(str)\n",
    "    freq = pd.concat([s_train, s_test]).value_counts(normalize=True)\n",
    "    train_df[f\"{col}_freq_all\"] = s_train.map(freq).fillna(0).astype(\"float32\")\n",
    "    test_df[f\"{col}_freq_all\"] = s_test.map(freq).fillna(0).astype(\"float32\")\n",
    "\n",
    "# Create clean copies\n",
    "train_df = train_df.copy()\n",
    "test_df = test_df.copy()\n",
    "\n",
    "\n",
    "# STEP 4: DATA CLEANING\n",
    "print(\"\\nStep 4: Removing empty and constant columns\")\n",
    "\n",
    "# Combine datasets to check for empty/constant columns\n",
    "all_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "cols_to_drop = []\n",
    "\n",
    "for c in train_df.columns:\n",
    "    if c in [LABEL_COL, ID_COL]:\n",
    "        continue\n",
    "\n",
    "    s = all_df[c]\n",
    "\n",
    "    # drop if all missing\n",
    "    if s.isna().all():\n",
    "        cols_to_drop.append(c)\n",
    "        continue\n",
    "\n",
    "    # drop if constant (including unhashable types)\n",
    "    try:\n",
    "        if s.nunique(dropna=False) <= 1:\n",
    "            cols_to_drop.append(c)\n",
    "    except TypeError:\n",
    "        continue\n",
    "\n",
    "print(f\"  Dropping {len(cols_to_drop)} empty or constant columns\")\n",
    "train_df.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n",
    "test_df.drop(columns=cols_to_drop, inplace=True, errors=\"ignore\")\n",
    "\n",
    "del all_df\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "# STEP 5: SELECT NUMERIC FEATURES\n",
    "\n",
    "print(\"\\nStep 5: Selecting numeric features for modeling\")\n",
    "\n",
    "# Columns to exclude from feature selection\n",
    "exclude_cols = [LABEL_COL, ID_COL, \"user_key\", \"challenge_id\"] + TEXT_COLS_EXCLUDE\n",
    "\n",
    "# Select only numeric columns (excluding IDs)\n",
    "feature_cols = [\n",
    "    c for c in train_df.columns\n",
    "    if c not in exclude_cols\n",
    "    and str(train_df[c].dtype) in [\"int64\", \"int32\", \"float64\", \"float32\", \"bool\"]\n",
    "    and \"id\" not in c.lower()\n",
    "]\n",
    "print(f\"  Selected {len(feature_cols)} tweet-level features\")\n",
    "\n",
    "\n",
    "# STEP 6: USER-LEVEL AGGREGATION\n",
    "\n",
    "print(\"\\nStep 6: Aggregating features to user level\")\n",
    "\n",
    "# create user key based on account creation time (proxy for unique user)\n",
    "train_df[\"user_key\"] = train_df[\"user.created_at\"].fillna(\"UNK\").astype(str)\n",
    "test_df[\"user_key\"] = test_df[\"user.created_at\"].fillna(\"UNK\").astype(str)\n",
    "\n",
    "# define aggregation functions\n",
    "agg_funcs = [\"mean\", \"max\", \"min\", \"std\"]\n",
    "agg_dict = {c: agg_funcs for c in feature_cols}\n",
    "agg_dict[ID_COL] = \"count\"  # count number of tweets per user\n",
    "\n",
    "# aggregate training data\n",
    "train_user = train_df.groupby(\"user_key\").agg(agg_dict)\n",
    "train_user.columns = [f\"{col}_{stat}\" for col, stat in train_user.columns.to_flat_index()]\n",
    "train_user = train_user.reset_index().rename(columns={f\"{ID_COL}_count\": \"n_tweets\"})\n",
    "\n",
    "# user-level labels (majority vote: 1 if mean >= 0.5, else 0)\n",
    "user_label = train_df.groupby(\"user_key\")[LABEL_COL].agg(\n",
    "    lambda x: 1 if x.mean() >= 0.5 else 0\n",
    ").reset_index()\n",
    "train_user = train_user.merge(user_label, on=\"user_key\")\n",
    "\n",
    "# aggregate test data\n",
    "test_user = test_df.groupby(\"user_key\").agg(agg_dict)\n",
    "test_user.columns = [f\"{col}_{stat}\" for col, stat in test_user.columns.to_flat_index()]\n",
    "test_user = test_user.reset_index().rename(columns={f\"{ID_COL}_count\": \"n_tweets\"})\n",
    "\n",
    "print(f\"  Train users: {len(train_user)}, Test users: {len(test_user)}\")\n",
    "\n",
    "# select user-level feature columns\n",
    "user_feature_cols = [c for c in train_user.columns if c not in [\"user_key\", LABEL_COL]]\n",
    "print(f\"  User-level features: {len(user_feature_cols)}\")\n",
    "\n",
    "\n",
    "# STEP 7: PREPARE FEATURE MATRICES\n",
    "\n",
    "print(\"\\nStep 7: Preparing feature matrices for training\")\n",
    "\n",
    "# extract labels\n",
    "y_user = train_user[LABEL_COL].astype(int).values\n",
    "\n",
    "# impute missing values and standardize features\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(\n",
    "    imputer.fit_transform(train_user[user_feature_cols].values)\n",
    ").astype(np.float32)\n",
    "\n",
    "X_test = scaler.transform(\n",
    "    imputer.transform(test_user[user_feature_cols].values)\n",
    ").astype(np.float32)\n",
    "\n",
    "print(f\"  X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "\n",
    "\n",
    "# STEP 8: TRAIN LIGHTGBM WITH CROSS-VALIDATION\n",
    "\n",
    "print(\"\\nStep 8: Training LightGBM with stratified k-fold cross-validation\")\n",
    "\n",
    "# Initialize stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "# Initialize out-of-fold predictions\n",
    "oof_user = np.zeros(len(train_user), dtype=np.float32)\n",
    "test_user_folds = []\n",
    "\n",
    "# Train model for each fold\n",
    "for fold, (tr_idx, val_idx) in enumerate(skf.split(X_train, y_user), 1):\n",
    "    print(f\"\\n  Fold {fold}/{N_FOLDS}\")\n",
    "\n",
    "    # Split data\n",
    "    X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y_user[tr_idx], y_user[val_idx]\n",
    "\n",
    "    # Train model with balanced class weights\n",
    "    model = LGBMClassifier(**LGBM_PARAMS)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        sample_weight=balanced_sample_weight(y_tr),\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric='binary_logloss'\n",
    "    )\n",
    "\n",
    "    # predict on validation set\n",
    "    val_proba = model.predict_proba(X_val)[:, 1]\n",
    "    oof_user[val_idx] = val_proba\n",
    "\n",
    "    # calculate and display fold accuracy\n",
    "    fold_acc = accuracy_score(y_val, (val_proba >= 0.5).astype(int))\n",
    "    print(f\"    Fold {fold} validation accuracy: {fold_acc:.4f}\")\n",
    "\n",
    "    # predict on test set\n",
    "    test_user_folds.append(model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    # clean up\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "# calculate overall out-of-fold accuracy\n",
    "oof_acc = accuracy_score(y_user, (oof_user >= 0.5).astype(int))\n",
    "print(f\"OVERALL USER-LEVEL OUT-OF-FOLD ACCURACY: {oof_acc:.4f}\")\n",
    "\n",
    "# average test predictions across folds\n",
    "test_user_mean = np.mean(test_user_folds, axis=0).astype(np.float32)\n",
    "\n",
    "\n",
    "# STEP 9: MAP USER-LEVEL PREDICTIONS BACK TO TWEETS\n",
    "\n",
    "print(\"\\nStep 9: Mapping user-level predictions back to tweet level\")\n",
    "\n",
    "# map OOF predictions to training tweets\n",
    "oof_tweet = train_df[[ID_COL, \"user_key\", LABEL_COL]].merge(\n",
    "    pd.DataFrame({\n",
    "        \"user_key\": train_user[\"user_key\"],\n",
    "        \"lightgbm_user_proba\": oof_user\n",
    "    }),\n",
    "    on=\"user_key\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# map test predictions to test tweets\n",
    "test_tweet = test_df[[ID_COL, \"user_key\"]].merge(\n",
    "    pd.DataFrame({\n",
    "        \"user_key\": test_user[\"user_key\"],\n",
    "        \"lightgbm_user_proba\": test_user_mean\n",
    "    }),\n",
    "    on=\"user_key\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# prepare user-level features for export\n",
    "lightgbm_features_train = oof_tweet[[ID_COL, \"user_key\"]].merge(\n",
    "    train_user.drop(columns=[LABEL_COL]),\n",
    "    on=\"user_key\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"user_key\"])\n",
    "\n",
    "lightgbm_features_test = test_tweet[[ID_COL, \"user_key\"]].merge(\n",
    "    test_user,\n",
    "    on=\"user_key\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"user_key\"])\n",
    "\n",
    "\n",
    "# STEP 10: SAVE OUTPUTS\n",
    "\n",
    "print(\"\\nStep 10: Saving outputs\")\n",
    "\n",
    "# save out-of-fold predictions\n",
    "oof_tweet[[ID_COL, LABEL_COL, \"lightgbm_user_proba\"]].to_csv(\n",
    "    os.path.join(OUT_DIR, \"oof_lgbm.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# save test predictions\n",
    "test_tweet[[ID_COL, \"lightgbm_user_proba\"]].to_csv(\n",
    "    os.path.join(OUT_DIR, \"test_lgbm.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "# save aggregated features for potential ensemble models\n",
    "lightgbm_features_train.to_csv(\n",
    "    os.path.join(OUT_DIR, \"lgbm_features_train.csv\"),\n",
    "    index=False\n",
    ")\n",
    "lightgbm_features_test.to_csv(\n",
    "    os.path.join(OUT_DIR, \"lgbm_features_test.csv\"),\n",
    "    index=False\n",
    ")\n",
    "\n",
    "print(f\"\\n  Out-of-fold predictions: {OUT_DIR}/oof_lgbm.csv\")\n",
    "print(f\"  Test predictions: {OUT_DIR}/test_lgbm.csv\")\n",
    "print(f\"  Aggregated features (train): {OUT_DIR}/lgbm_features_train.csv\")\n",
    "print(f\"  Aggregated features (test):  {OUT_DIR}/lgbm_features_test.csv\")\n",
    "print(\"\\nPipeline completed!\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading JSONL data\n",
      "Train shape: (154914, 194), Test shape: (103380, 192)\n",
      "\n",
      "Step 2: Engineering structural features\n",
      "  - Temporal features (hour, weekday, month, year)\n",
      "  - Account age features\n",
      "  - Text-based features (length, hashtags, mentions)\n",
      "  - User description features\n",
      "  - Presence flags (location, banner, profile URL)\n",
      "  - Entity counts (hashtags, mentions, URLs, symbols)\n",
      "  - Profile color RGB components\n",
      "  - Source frequency encoding\n",
      "  - User URL domain frequency\n",
      "  - Withheld countries features\n",
      "  - Coordinate presence flags\n",
      "  - Quoted status temporal features\n",
      "  - Quoted status numeric features and log transforms\n",
      "  - Global mapping quoted user friends/followers to main users via created_at\n",
      "\n",
      "Step 3: Processing generic object columns\n",
      "  Processing 131 object columns\n",
      "  - Counting items in list-like columns\n",
      "  - Additional frequency encodings for categorical columns\n",
      "\n",
      "Step 4: Removing empty and constant columns\n",
      "  Dropping 78 empty or constant columns\n",
      "\n",
      "Step 5: Selecting numeric features for modeling\n",
      "  Selected 343 tweet-level features\n",
      "\n",
      "Step 6: Aggregating features to user level\n",
      "  Train users: 30696, Test users: 20465\n",
      "  User-level features: 1373\n",
      "\n",
      "Step 7: Preparing feature matrices for training\n",
      "  X_train shape: (30696, 1372), X_test shape: (20465, 1372)\n",
      "\n",
      "Step 8: Training LightGBM with stratified k-fold cross-validation\n",
      "\n",
      "  Fold 1/5\n",
      "    Fold 1 validation accuracy: 0.8686\n",
      "\n",
      "  Fold 2/5\n",
      "    Fold 2 validation accuracy: 0.8708\n",
      "\n",
      "  Fold 3/5\n",
      "    Fold 3 validation accuracy: 0.8681\n",
      "\n",
      "  Fold 4/5\n",
      "    Fold 4 validation accuracy: 0.8760\n",
      "\n",
      "  Fold 5/5\n",
      "    Fold 5 validation accuracy: 0.8698\n",
      "OVERALL USER-LEVEL OUT-OF-FOLD ACCURACY: 0.8707\n",
      "\n",
      "Step 9: Mapping user-level predictions back to tweet level\n",
      "\n",
      "Step 10: Saving outputs\n",
      "\n",
      "  Out-of-fold predictions: ./intermediate/oof_lgbm.csv\n",
      "  Test predictions: ./intermediate/test_lgbm.csv\n",
      "  Aggregated features (train): ./intermediate/lgbm_features_train.csv\n",
      "  Aggregated features (test):  ./intermediate/lgbm_features_test.csv\n",
      "\n",
      "Pipeline completed!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R7LGM0v2BITd"
   },
   "source": [
    "```text\n",
    "Step 1: Loading JSONL data\n",
    "Train shape: (154914, 194), Test shape: (103380, 192)\n",
    "\n",
    "Step 2: Engineering structural features\n",
    "  - Temporal features (hour, weekday, month, year)\n",
    "  - Account age features\n",
    "  - Text-based features (length, hashtags, mentions)\n",
    "  - User description features\n",
    "  - Presence flags (location, banner, profile URL)\n",
    "  - Entity counts (hashtags, mentions, URLs, symbols)\n",
    "  - Profile color RGB components\n",
    "  - Source frequency encoding\n",
    "  - User URL domain frequency\n",
    "  - Withheld countries features\n",
    "  - Coordinate presence flags\n",
    "  - Quoted status temporal features\n",
    "  - Quoted status numeric features and log transforms\n",
    "  - Global mapping quoted user friends/followers to main users via created_at\n",
    "\n",
    "Step 3: Processing generic object columns\n",
    "  Processing 131 object columns\n",
    "  - Counting items in list-like columns\n",
    "  - Additional frequency encodings for categorical columns\n",
    "\n",
    "Step 4: Removing empty and constant columns\n",
    "  Dropping 78 empty or constant columns\n",
    "\n",
    "Step 5: Selecting numeric features for modeling\n",
    "  Selected 343 tweet-level features\n",
    "\n",
    "Step 6: Aggregating features to user level\n",
    "  Train users: 30696, Test users: 20465\n",
    "  User-level features: 1373\n",
    "\n",
    "Step 7: Preparing feature matrices for training\n",
    "  X_train shape: (30696, 1372), X_test shape: (20465, 1372)\n",
    "\n",
    "Step 8: Training LightGBM with stratified k-fold cross-validation\n",
    "\n",
    "  Fold 1/5\n",
    "    Fold 1 validation accuracy: 0.8686\n",
    "\n",
    "  Fold 2/5\n",
    "    Fold 2 validation accuracy: 0.8708\n",
    "\n",
    "  Fold 3/5\n",
    "    Fold 3 validation accuracy: 0.8681\n",
    "\n",
    "  Fold 4/5\n",
    "    Fold 4 validation accuracy: 0.8760\n",
    "\n",
    "  Fold 5/5\n",
    "    Fold 5 validation accuracy: 0.8698\n",
    "\n",
    "OVERALL USER-LEVEL OUT-OF-FOLD ACCURACY: 0.8707\n",
    "\n",
    "Step 9: Mapping user-level predictions back to tweet level\n",
    "\n",
    "Step 10: Saving outputs\n",
    "\n",
    "  Out-of-fold predictions: ./intermediate/oof_lgbm.csv\n",
    "  Test predictions: ./intermediate/test_lgbm.csv\n",
    "  Aggregated features (train): ./intermediate/lgbm_features_train.csv\n",
    "  Aggregated features (test):  ./intermediate/lgbm_features_test.csv\n",
    "\n",
    "Pipeline completed!\n",
    "```\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
