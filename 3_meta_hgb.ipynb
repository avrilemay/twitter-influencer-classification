{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "L4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3_meta_hgb.ipynb\n",
    "\n",
    "Final stacking layer combining base models (LightGBM + CamemBERTav2).\n",
    "\n",
    "  1. Aggregates OOF predictions & engineers consensus features (vote sum, disagreement).\n",
    "  2. Computes user-level stability stats.\n",
    "  3. Trains a HistGradientBoosting meta-model using StratifiedGroupKFold to prevent leakage.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "\n",
    "# configuration\n",
    "\n",
    "SEED = 42\n",
    "N_FOLDS = 5\n",
    "\n",
    "# setup directories\n",
    "# ROOT_DIR = \"/content/drive/MyDrive/Colab Notebooks/code\" # for google colab\n",
    "ROOT_DIR = \".\"\n",
    "OUT_DIR = os.path.join(ROOT_DIR, \"intermediate\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# input raw data files\n",
    "TRAIN_JSONL = os.path.join(ROOT_DIR, \"train.jsonl\")\n",
    "TEST_JSONL = os.path.join(ROOT_DIR, \"kaggle_test.jsonl\")\n",
    "\n",
    "# model outputs (predictions from previous steps)\n",
    "# --> input features for the meta-model (stacking)\n",
    "OOF_LGBM_USER_FILE = os.path.join(OUT_DIR, \"oof_lgbm.csv\")\n",
    "TEST_LGBM_USER_FILE = os.path.join(OUT_DIR, \"test_lgbm.csv\")\n",
    "LGBM_FEATS_TRAIN_USER_FILE = os.path.join(OUT_DIR, \"lgbm_features_train.csv\")\n",
    "LGBM_FEATS_TEST_USER_FILE = os.path.join(OUT_DIR, \"lgbm_features_test.csv\")\n",
    "\n",
    "OOF_CAMEMBERT_FILE = os.path.join(OUT_DIR, \"oof_camembert.csv\")\n",
    "TEST_CAMEMBERT_FILE = os.path.join(OUT_DIR, \"test_camembert.csv\")\n",
    "\n",
    "# final submission file\n",
    "SUB_OUT = os.path.join(ROOT_DIR, \"submission.csv\")\n",
    "\n",
    "ID_COL, LABEL_COL = \"ID\", \"label\"\n",
    "\n",
    "\n",
    "# load data and build user key\n",
    "print(\"Loading JSONL files and creating user keys\")\n",
    "\n",
    "# load raw jsonl data\n",
    "raw_train = pd.read_json(TRAIN_JSONL, lines=True)\n",
    "raw_test = pd.read_json(TEST_JSONL, lines=True)\n",
    "\n",
    "# flatten nested json objects into a pandas dataframe\n",
    "train_df_full = json_normalize(raw_train.to_dict(orient='records'))\n",
    "test_df_full = json_normalize(raw_test.to_dict(orient='records'))\n",
    "\n",
    "# create a mapping to link challenges to users\n",
    "# we use 'user.created_at' as a proxy for user id since explicit ids are missing\n",
    "user_train_map = (\n",
    "    train_df_full[[\"challenge_id\", \"user.created_at\"]]\n",
    "    .rename(columns={\"challenge_id\": ID_COL})\n",
    ")\n",
    "user_test_map = (\n",
    "    test_df_full[[\"challenge_id\", \"user.created_at\"]]\n",
    "    .rename(columns={\"challenge_id\": ID_COL})\n",
    ")\n",
    "\n",
    "# convert to string to ensure consistent matching later\n",
    "user_train_map[\"user_key\"] = user_train_map[\"user.created_at\"].astype(str)\n",
    "user_test_map[\"user_key\"] = user_test_map[\"user.created_at\"].astype(str)\n",
    "\n",
    "print(f\"Unique users in train: {user_train_map['user_key'].nunique()}\")\n",
    "print(f\"Unique users in test:  {user_test_map['user_key'].nunique()}\")\n",
    "\n",
    "\n",
    "# load base model outputs (stacking layer 1)\n",
    "# we load predictions made by base models (lightgbm, camembertav2)\n",
    "# these probabilities become the input features (x) for our meta-model\n",
    "print(\"Loading LightGBM user-level predictions\")\n",
    "oof_lgbm_user = pd.read_csv(OOF_LGBM_USER_FILE)\n",
    "test_lgbm_user = pd.read_csv(TEST_LGBM_USER_FILE)\n",
    "\n",
    "# create working copies\n",
    "oof_train = oof_lgbm_user.copy()\n",
    "test_train = test_lgbm_user.copy()\n",
    "\n",
    "print(\"Loading CamemBERTa predictions\")\n",
    "camembert_trn_map, camembert_tst_map = None, None\n",
    "\n",
    "# try-except blocks\n",
    "# if a specific model output is missing, the code continues without it\n",
    "try:\n",
    "    camembert_trn = pd.read_csv(OOF_CAMEMBERT_FILE).drop(columns=[\"user_label\"], errors=\"ignore\")\n",
    "    # rename column for clarity and to avoid conflicts with lgbm cols\n",
    "    camembert_trn = camembert_trn.rename(columns={\"oof_proba\": \"camembert_proba\"})\n",
    "    # merge camemberta probs onto the user map to align data\n",
    "    camembert_trn_map = user_train_map.merge(\n",
    "        camembert_trn[[\"user_key\", \"camembert_proba\"]],\n",
    "        on=\"user_key\", how=\"left\"\n",
    "    )\n",
    "except:\n",
    "    print(\"CamemBERTa OOF file not found - proceeding without it\")\n",
    "\n",
    "try:\n",
    "    camembert_tst = pd.read_csv(TEST_CAMEMBERT_FILE)\n",
    "    camembert_tst_map = user_test_map.merge(\n",
    "        camembert_tst[[\"user_key\", \"camembert_proba\"]],\n",
    "        on=\"user_key\", how=\"left\"\n",
    "    )\n",
    "except:\n",
    "    print(\"CamemBERTa test file not found - proceeding without it\")\n",
    "\n",
    "\n",
    "print(\"Loading LightGBM user features\")\n",
    "try:\n",
    "    lgbm_feats_trn = pd.read_csv(LGBM_FEATS_TRAIN_USER_FILE)\n",
    "    lgbm_feats_tst = pd.read_csv(LGBM_FEATS_TEST_USER_FILE)\n",
    "except:\n",
    "    lgbm_feats_trn = lgbm_feats_tst = None\n",
    "    print(\"LightGBM feature files not found\")\n",
    "\n",
    "# merge all features\n",
    "# consolidate all signals (model predictions + engineered features) into one dataframe per split\n",
    "print(\"Merging features\")\n",
    "\n",
    "y = oof_train[LABEL_COL].values\n",
    "train_merged = oof_train.copy()\n",
    "test_merged = test_train.copy()\n",
    "\n",
    "# merge pre-calculated lightgbm features if available\n",
    "if lgbm_feats_trn is not None:\n",
    "    train_merged = train_merged.merge(lgbm_feats_trn, on=ID_COL, how=\"left\")\n",
    "if lgbm_feats_tst is not None:\n",
    "    test_merged = test_merged.merge(lgbm_feats_tst, on=ID_COL, how=\"left\")\n",
    "\n",
    "# merge camemberta probabilities if available\n",
    "if camembert_trn_map is not None:\n",
    "    train_merged = train_merged.merge(\n",
    "        camembert_trn_map[[ID_COL, \"camembert_proba\"]], on=ID_COL, how=\"left\"\n",
    "    )\n",
    "if camembert_tst_map is not None:\n",
    "    test_merged = test_merged.merge(\n",
    "        camembert_tst_map[[ID_COL, \"camembert_proba\"]], on=ID_COL, how=\"left\"\n",
    "    )\n",
    "\n",
    "# finally, add the user keys back to the merged data for grouping\n",
    "train_merged = train_merged.merge(user_train_map[[ID_COL, \"user_key\"]], on=ID_COL, how=\"left\")\n",
    "test_merged = test_merged.merge(user_test_map[[ID_COL, \"user_key\"]], on=ID_COL, how=\"left\")\n",
    "\n",
    "\n",
    "# meta feature engineering\n",
    "# create new features based on the relationship between model predictions\n",
    "# e.g. do the models agree? what is the mean confidence?\n",
    "\n",
    "print(\"Creating meta-features\")\n",
    "\n",
    "proba_cols = [\"lightgbm_user_proba\"]\n",
    "if \"camembert_proba\" in train_merged.columns:\n",
    "    proba_cols.append(\"camembert_proba\")\n",
    "\n",
    "# calculate statistics across the ensemble members\n",
    "train_merged[\"models_mean_proba\"] = train_merged[proba_cols].mean(axis=1)\n",
    "train_merged[\"models_std_proba\"] = train_merged[proba_cols].std(axis=1) # high std = models disagree\n",
    "test_merged[\"models_mean_proba\"] = test_merged[proba_cols].mean(axis=1)\n",
    "test_merged[\"models_std_proba\"] = test_merged[proba_cols].std(axis=1)\n",
    "\n",
    "# convert probabilities to hard votes (0 or 1)\n",
    "for col in proba_cols:\n",
    "    train_merged[f\"hard_{col}\"] = (train_merged[col] > 0.5).astype(int)\n",
    "    test_merged[f\"hard_{col}\"] = (test_merged[col] > 0.5).astype(int)\n",
    "\n",
    "# sum the votes to see consensus strength [0 or 1 or 2 ]\n",
    "train_merged[\"models_vote_sum\"] = train_merged[[f\"hard_{c}\" for c in proba_cols]].sum(axis=1)\n",
    "test_merged[\"models_vote_sum\"] = test_merged[[f\"hard_{c}\" for c in proba_cols]].sum(axis=1)\n",
    "\n",
    "# calculate absolute difference between specific models\n",
    "if \"camembert_proba\" in train_merged.columns:\n",
    "    train_merged[\"delta_proba_lgbm_camembert\"] = (\n",
    "        train_merged[\"lightgbm_user_proba\"] - train_merged[\"camembert_proba\"]\n",
    "    ).abs()\n",
    "    test_merged[\"delta_proba_lgbm_camembert\"] = (\n",
    "        test_merged[\"lightgbm_user_proba\"] - test_merged[\"camembert_proba\"]\n",
    "    ).abs()\n",
    "\n",
    "# user-level statistics --> aggregate predictions per user\n",
    "# is a user generally \"easy\" or \"hard\" to classify ?\n",
    "for name, df_sub in [(\"train\", train_merged), (\"test\", test_merged)]:\n",
    "    # compute mean, max, std of probabilities grouped by user\n",
    "    stats = df_sub.groupby(\"user_key\")[proba_cols].agg(\n",
    "        {c: [\"mean\", \"max\", \"std\"] for c in proba_cols}\n",
    "    )\n",
    "    # flatten multi-level column names (e.g. ('proba', 'mean') -> 'user_proba_mean')\n",
    "    stats.columns = [f\"user_{c}_{s}\" for c, s in stats.columns.to_flat_index()]\n",
    "    stats = stats.reset_index().fillna(0)\n",
    "\n",
    "    if name == \"train\":\n",
    "        train_merged = train_merged.merge(stats, on=\"user_key\", how=\"left\")\n",
    "    else:\n",
    "        test_merged = test_merged.merge(stats, on=\"user_key\", how=\"left\")\n",
    "\n",
    "# feature selection\n",
    "print(\"Selecting numeric features\")\n",
    "\n",
    "# define what not to train on (identifiers, strings, target label)\n",
    "exclude_cols = {ID_COL, LABEL_COL, \"user_key\", \"user.created_at\"}\n",
    "\n",
    "# automatically select all numeric columns available\n",
    "numeric_cols = set(train_merged.select_dtypes(include=[np.number]).columns)\n",
    "feature_cols = sorted(numeric_cols - exclude_cols)\n",
    "\n",
    "# prepare numpy arrays for training\n",
    "X_train = train_merged[feature_cols].values\n",
    "X_test = test_merged[feature_cols].values\n",
    "\n",
    "# meta-model training\n",
    "print(\"Training meta-model\")\n",
    "\n",
    "# using HGB which handles nans natively\n",
    "hgb_params = {\n",
    "    \"learning_rate\": 0.03, # step size, lower is more robust\n",
    "    \"max_iter\": 800, # max number of trees\n",
    "    \"max_depth\": 6, # limits complexity to avoid overfitting\n",
    "    \"min_samples_leaf\": 60, # prevents fitting to noise/outliers\n",
    "    \"l2_regularization\": 0.5, # penalty on weights to smooth model\n",
    "    \"max_bins\": 255, # feature discretization precision\n",
    "    \"random_state\": SEED, # reproducibility\n",
    "    \"early_stopping\": True, # auto-stop if validation stagnates\n",
    "    \"validation_fraction\": 0.10, # % of data used to check early stopping\n",
    "    \"n_iter_no_change\": 30, # patience buffer\n",
    "}\n",
    "\n",
    "# handling the groups for cross-validation\n",
    "# we fill nas just to avoid errors, though user_key shouldn't be na\n",
    "groups = train_merged[\"user_key\"].fillna(\"NA_USER\").values\n",
    "\n",
    "# stratifiedgroupkfold\n",
    "# 1. stratified: keeps the ratio of class 0 vs class 1 constant in each fold\n",
    "# 2. group: ensures all samples from the same user are in the same fold (train OR val)\n",
    "cv = StratifiedGroupKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof_proba = np.zeros(len(y))\n",
    "test_proba_folds = []\n",
    "\n",
    "# main training loop\n",
    "for fold, (tr_idx, val_idx) in enumerate(cv.split(X_train, y, groups), 1):\n",
    "    X_tr, X_val = X_train[tr_idx], X_train[val_idx]\n",
    "    y_tr, y_val = y[tr_idx], y[val_idx]\n",
    "\n",
    "    clf = HistGradientBoostingClassifier(**hgb_params)\n",
    "    clf.fit(X_tr, y_tr)\n",
    "\n",
    "    # predict probability of class 1\n",
    "    val_pred = clf.predict_proba(X_val)[:, 1]\n",
    "    oof_proba[val_idx] = val_pred\n",
    "\n",
    "    # simple accuracy check for monitoring\n",
    "    acc = accuracy_score(y_val, (val_pred >= 0.5).astype(int))\n",
    "    print(f\"Fold {fold}: accuracy = {acc:.4f}\")\n",
    "\n",
    "    # predict on the test set for this fold\n",
    "    test_proba_folds.append(clf.predict_proba(X_test)[:, 1])\n",
    "\n",
    "# global oof accuracy\n",
    "total_acc = accuracy_score(y, (oof_proba >= 0.5).astype(int))\n",
    "print(f\"> Overall OOF Accuracy: {total_acc:.4f} <\")\n",
    "\n",
    "# generate submission\n",
    "print(\"Exporting submission\")\n",
    "\n",
    "# average the predictions from all folds (bagging) to improve stability\n",
    "test_proba_mean = np.mean(test_proba_folds, axis=0)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_merged[ID_COL].values,\n",
    "    # threshold the probability at 0.5 to get binary class\n",
    "    \"Predicted\": (test_proba_mean >= 0.5).astype(int),\n",
    "})\n",
    "submission.to_csv(SUB_OUT, index=False)\n",
    "\n",
    "print(f\"Submission written to {SUB_OUT}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5QiJh2QMM0iJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765069894298,
     "user_tz": -60,
     "elapsed": 851669,
     "user": {
      "displayName": "Avrile Floro",
      "userId": "06557886888439066437"
     }
    },
    "outputId": "718c6534-47b3-4b7d-9efa-57d3e357bd99",
    "ExecuteTime": {
     "end_time": "2025-12-11T09:59:10.643563Z",
     "start_time": "2025-12-11T09:48:04.433089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading JSONL files and creating user keys\n",
      "Unique users in train: 30696\n",
      "Unique users in test:  20465\n",
      "Loading LightGBM user-level predictions\n",
      "Loading CamemBERTa predictions\n",
      "Loading LightGBM user features\n",
      "Merging features\n",
      "Creating meta-features\n",
      "Selecting numeric features\n",
      "Training meta-model\n",
      "Fold 1: accuracy = 0.8716\n",
      "Fold 2: accuracy = 0.8723\n",
      "Fold 3: accuracy = 0.8837\n",
      "Fold 4: accuracy = 0.8809\n",
      "Fold 5: accuracy = 0.8777\n",
      "> Overall OOF Accuracy: 0.8772 <\n",
      "Exporting submission\n",
      "Submission written to ./submission.csv\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```text\n",
    "Loading JSONL files and creating user keys\n",
    "Unique users in train: 30696\n",
    "Unique users in test:  20465\n",
    "\n",
    "Loading LightGBM user-level predictions\n",
    "Loading CamemBERT predictions\n",
    "Loading LightGBM user features\n",
    "\n",
    "Merging features\n",
    "Creating meta-features\n",
    "Selecting numeric features\n",
    "\n",
    "Training meta-model\n",
    "Fold 1: accuracy = 0.8716\n",
    "Fold 2: accuracy = 0.8723\n",
    "Fold 3: accuracy = 0.8837\n",
    "Fold 4: accuracy = 0.8809\n",
    "Fold 5: accuracy = 0.8777\n",
    "\n",
    "> Overall OOF Accuracy: 0.8772 <\n",
    "\n",
    "Exporting submission\n",
    "Submission written to ./submission.csv\n",
    "```\n"
   ]
  }
 ]
}
