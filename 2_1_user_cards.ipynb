{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "A100"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "2_1_user_cards.ipynb\n",
    "\n",
    "This script builds **user cards** for a binary classification task (*influencer vs observer*).\n",
    "\n",
    "- Aggregates multiple tweets per user into a single text card (profile + behavior + tweet samples)\n",
    "- Includes profile metadata (bio, location, account age, friends/followers via quoted_status)\n",
    "- Adds behavioral stats (share of original tweets vs replies)\n",
    "- Samples 2–3 representative tweets per card\n",
    "- Uses a multi-card strategy for users with many tweets (1–2 cards per user)\n",
    "- Cleans URLs to compact placeholders like `[LINK: domain]` to reduce noise\n",
    "\n",
    "These user cards are then used as inputs to an encoder for user-level classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "\n",
    "# config\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# paths\n",
    "ROOT_DIR = \".\"\n",
    "OUT_DIR = os.path.join(ROOT_DIR, \"intermediate\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# inputs\n",
    "TRAIN_PATH = os.path.join(ROOT_DIR, \"train.jsonl\")\n",
    "TEST_PATH = os.path.join(ROOT_DIR, \"kaggle_test.jsonl\")\n",
    "\n",
    "# outputs\n",
    "OUT_TRAIN = os.path.join(OUT_DIR, \"user_cards_train.csv\")\n",
    "OUT_TEST = os.path.join(OUT_DIR, \"user_cards_test.csv\")\n",
    "\n",
    "# twitter dates come in this specific format: \"Wed Oct 10 20:19:24 +0000 2018\"\n",
    "TWITTER_DT_FORMAT = \"%a %b %d %H:%M:%S %z %Y\"\n",
    "\n",
    "# card gen params\n",
    "\n",
    "# how many tweets to show the model per single card\n",
    "TWEETS_PER_CARD = 3\n",
    "BIO_MAX_LEN = 160 # standard twitter bio limit\n",
    "TWEET_MAX_LEN = 320 # roughly double the old limit to be safe\n",
    "PROMPT_MAX_TOKENS = 512\n",
    "MIN_TWEETS_FOR_TWO_CARDS = 5 # if user has enough data, we split into 2 cards for augmentation\n",
    "LABEL_COL = \"label\"\n",
    "\n",
    "# utils\n",
    "\n",
    "# pre-compile regex for whitespace cleanup to speed up processing\n",
    "_re_ws = re.compile(r\"\\s+\")\n",
    "\n",
    "\n",
    "def sanitize_text(s, max_len=None):\n",
    "    \"\"\"\n",
    "    performs basic text cleanup by collapsing multiple spaces and handling null inputs\n",
    "    optionally truncates the result if a max length is provided\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str): # safety check for non-string junk data\n",
    "        s = \"\" if s is None else str(s)\n",
    "\n",
    "    cleaned = _re_ws.sub(\" \", s).strip() # replaces newlines/tabs with single space\n",
    "\n",
    "    if max_len is not None: # hard cut if text exceeds limit\n",
    "        cleaned = cleaned[:max_len]\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def extract_domain(url):\n",
    "    \"\"\"\n",
    "    extracts the main domain name from a given url string\n",
    "    returns none if the url is invalid or too short\n",
    "    \"\"\"\n",
    "    # basic length check to filter out broken strings\n",
    "    if not isinstance(url, str) or len(url) < 5:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        # uses urlparse to cleanly get the network location\n",
    "        dom = urlparse(url).netloc\n",
    "\n",
    "        if dom.startswith(\"www.\"):  # strip 'www.' to standardize domains\n",
    "            return dom[4:].lower()\n",
    "        elif dom:\n",
    "            return dom.lower()\n",
    "        else:\n",
    "            return None\n",
    "    except:\n",
    "        # failsafe for any parsing errors\n",
    "        return None\n",
    "\n",
    "\n",
    "def clean_tweet_urls(text):\n",
    "    \"\"\"\n",
    "    scans tweet text and replaces specific urls with a simplified [LINK: domain] tag\n",
    "    specifically handles t.co links which are uninformative on their own\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return text\n",
    "\n",
    "    def replace_url(match):\n",
    "        \"\"\"\n",
    "        internal callback to determine how to replace each found url\n",
    "        \"\"\"\n",
    "        url = match.group(0)\n",
    "\n",
    "        # t.co is twitter's shortener so it doesn't tell us the destination -> generic tag\n",
    "        if url.startswith(\"t.co\") or \"t.co/\" in url:\n",
    "            return \"[LINK]\"\n",
    "\n",
    "        # for real urls, we try to show the domain to give context to the model\n",
    "        domain = extract_domain(url)\n",
    "        if domain:\n",
    "            return f\"[LINK: {domain}]\"\n",
    "        else:\n",
    "            # fallback if domain extraction fails\n",
    "            return \"[LINK]\"\n",
    "\n",
    "    # regex matches standard http/https, bare www, or t.co links\n",
    "    text = re.sub(\n",
    "        r'https?://\\S+|(?:www\\.)\\S+|t\\.co/\\S+',\n",
    "        replace_url,\n",
    "        text\n",
    "    )\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_core_tweet_text(row):\n",
    "    \"\"\"\n",
    "    retrieves the actual text content from the tweet object\n",
    "    prioritizes the 'extended_tweet' field which contains the full un-truncated text\n",
    "    \"\"\"\n",
    "    # check columns in order of information quality\n",
    "    for col in [\"extended_tweet.full_text\", \"full_text\", \"text\"]:\n",
    "        # verify the column exists and has valid content\n",
    "        if col in row and isinstance(row[col], str) and row[col].strip():\n",
    "            return row[col]\n",
    "\n",
    "    # return empty if no valid text found\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def build_user_key(df):\n",
    "    \"\"\"\n",
    "    generates a unique user key based on account creation date\n",
    "    sorts the dataframe to ensure deterministic processing later\n",
    "    \"\"\"\n",
    "    user_col = \"user.created_at\"\n",
    "\n",
    "    # sorting is required for mergesort stability and grouping logic\n",
    "    df.sort_values(by=[user_col], inplace=True, kind=\"mergesort\")\n",
    "\n",
    "    # create a string key from the creation date, filling nans\n",
    "    df[\"user_key\"] = df[user_col].fillna(\"NA\").astype(str)\n",
    "\n",
    "\n",
    "def majority_label(labels):\n",
    "    \"\"\"\n",
    "    calculates the majority class (0 or 1) for a set of user labels\n",
    "    handles cases where a user might have conflicting labels in the raw data\n",
    "    \"\"\"\n",
    "    # convert to numeric and drop bad data\n",
    "    vals = pd.to_numeric(labels, errors=\"coerce\").dropna().values\n",
    "\n",
    "    if len(vals) > 0: # average the labels and round to get the majority class\n",
    "        return int(round(vals.mean()))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def get_behavioral_type(row):\n",
    "    \"\"\"\n",
    "    classifies a single tweet into one of three behavioral categories\n",
    "    checks specific twitter api fields to distinguish quotes, replies, and originals\n",
    "    \"\"\"\n",
    "    # quote status takes precedence as it involves sharing content\n",
    "    if bool(row.get(\"is_quote_status\")) or pd.notna(row.get(\"quoted_status_id_str\")):\n",
    "        return \"quote\"\n",
    "\n",
    "    # check if it is a direct reply to another user\n",
    "    if pd.notna(row.get(\"in_reply_to_user_id_str\")):\n",
    "        return \"reply\"\n",
    "\n",
    "    # if neither, it is considered original content\n",
    "    return \"original\"\n",
    "\n",
    "\n",
    "def compute_behavior_stats(tweet_types):\n",
    "    \"\"\"\n",
    "    calculates the percentage distribution of original tweets vs replies\n",
    "    note that quotes are counted in the total but not explicitly returned\n",
    "    \"\"\"\n",
    "    if not tweet_types:\n",
    "        return {\"pct_original\": 0.0, \"pct_reply\": 0.0}\n",
    "\n",
    "    # efficient counting of types\n",
    "    counts = Counter(tweet_types)\n",
    "    total = len(tweet_types)\n",
    "\n",
    "    n_original = counts.get(\"original\", 0)\n",
    "    n_reply = counts.get(\"reply\", 0)\n",
    "\n",
    "    return {\n",
    "        \"pct_original\": n_original / total * 100.0,\n",
    "        \"pct_reply\": n_reply / total * 100.0,\n",
    "    }\n",
    "\n",
    "\n",
    "# card generation\n",
    "\n",
    "def generate_user_cards(df, is_train=True):\n",
    "    \"\"\"\n",
    "    orchestrates the creation of 'user cards' for the model\n",
    "    extracts metadata, computes stats, samples tweets, and builds the final prompt\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "\n",
    "    # pre-calculate tweet types for the entire dataset for efficiency\n",
    "    df[\"tweet_type\"] = df.apply(get_behavioral_type, axis=1)\n",
    "\n",
    "    # try to extract a domain from the user's profile url field\n",
    "    if \"user.url\" in df.columns:\n",
    "        df[\"user_profile_domain\"] = df[\"user.url\"].map(extract_domain)\n",
    "    else:\n",
    "        df[\"user_profile_domain\"] = None\n",
    "\n",
    "    # process each user group individually, maintaining original sort order\n",
    "    for user_key, g in df.groupby(\"user_key\", sort=False):\n",
    "        # reset index to handle random sampling cleanly\n",
    "        g = g.reset_index(drop=True)\n",
    "        n_tweets = len(g)\n",
    "\n",
    "        #  extract metadata\n",
    "\n",
    "        def first_non_null(col):\n",
    "            \"\"\"helper to grab the first valid value for user-level fields (for each user, only need 1 value\n",
    "            if some are missing)\"\"\"\n",
    "            if col not in g.columns:\n",
    "                return None\n",
    "            vals = g[col].dropna()\n",
    "            return vals.iloc[0] if len(vals) > 0 else None\n",
    "\n",
    "        def get_max_int(col):\n",
    "            \"\"\"helper to grab the maximum value seen for counters like follower count\"\"\"\n",
    "            if col not in g.columns:\n",
    "                return 0\n",
    "            vals = pd.to_numeric(g[col], errors=\"coerce\").dropna()\n",
    "            return int(vals.max()) if len(vals) > 0 else 0\n",
    "\n",
    "        # clean up bio and location text\n",
    "        user_desc = sanitize_text(first_non_null(\"user.description\") or \"\", max_len=BIO_MAX_LEN)\n",
    "        user_location = sanitize_text(first_non_null(\"user.location\") or \"\", max_len=50)\n",
    "\n",
    "        # gather key activity metrics\n",
    "        listed_count = get_max_int(\"user.listed_count\")\n",
    "        statuses_count = get_max_int(\"user.statuses_count\")\n",
    "        favourites_count = get_max_int(\"user.favourites_count\")\n",
    "\n",
    "        # retrieve inferred stats from the global quoted mapping (computed in main)\n",
    "        quoted_friends = first_non_null(\"user_friends_from_quoted\")\n",
    "        quoted_followers = first_non_null(\"user_followers_from_quoted\")\n",
    "\n",
    "        #  calc account age\n",
    "        user_created = first_non_null(\"user_created_at_dt\")\n",
    "        last_tweet_dt = g[\"tweet_datetime\"].max()\n",
    "\n",
    "        # calculate age in years to give the model context on account maturity\n",
    "        age_str = \"Unknown\"\n",
    "        if pd.notna(user_created) and pd.notna(last_tweet_dt):\n",
    "            age_days = (last_tweet_dt - user_created).days\n",
    "            if age_days >= 0:\n",
    "                age_str = f\"{age_days / 365.25:.1f}y\"\n",
    "\n",
    "        #  profile links\n",
    "        # dedup domains using a set (user might link same site multiple times)\n",
    "        domains = {g[\"user_profile_domain\"].iloc[0]} if g[\"user_profile_domain\"].iloc[0] else set()\n",
    "        links_str = \", \".join(list(domains)[:3]) if domains else \"\"\n",
    "\n",
    "\n",
    "        #  behavior stats\n",
    "        behavior_stats = compute_behavior_stats(g[\"tweet_type\"].tolist())\n",
    "\n",
    "        #  user label\n",
    "        # compute the ground truth label only if we are in training mode\n",
    "        user_label = majority_label(g[LABEL_COL]) if is_train else None\n",
    "\n",
    "        #  card logic\n",
    "        # data augmentation: if user has enough tweets, generate 2 different cards\n",
    "        n_cards = 1 if n_tweets < MIN_TWEETS_FOR_TWO_CARDS else 2\n",
    "\n",
    "        # determine distribution of tweets across cards\n",
    "        if n_cards == 1:\n",
    "            tweets_per_card_list = [min(n_tweets, TWEETS_PER_CARD)]\n",
    "        else:\n",
    "            # card 1 is full, card 2 takes the remainder (with a minimum of 2)\n",
    "            tweets_per_card_list = [\n",
    "                TWEETS_PER_CARD,\n",
    "                min(TWEETS_PER_CARD, max(2, n_tweets - TWEETS_PER_CARD))\n",
    "            ]\n",
    "\n",
    "        used_indices = set()\n",
    "\n",
    "        # loop over cards\n",
    "        for card_id in range(n_cards):\n",
    "            # filter out tweets already used in previous cards for this user\n",
    "            available = [i for i in range(n_tweets) if i not in used_indices]\n",
    "\n",
    "            # failsafe: if we ran out of unique tweets, reset pool\n",
    "            if not available:\n",
    "                available = list(range(n_tweets))\n",
    "\n",
    "            n_select = min(tweets_per_card_list[card_id], len(available))\n",
    "\n",
    "            # randomly select tweets for this card\n",
    "            selected_indices = list(np.random.choice(len(available), size=n_select, replace=False))\n",
    "            selected_indices = [available[i] for i in selected_indices]\n",
    "\n",
    "            # mark selected tweets as used\n",
    "            used_indices.update(selected_indices)\n",
    "\n",
    "            # slice the dataframe to get the specific tweets\n",
    "            selected_tweets = g.iloc[selected_indices]\n",
    "\n",
    "            #  build prompt\n",
    "\n",
    "            lines = [\n",
    "                \"## USER PROFILE ANALYSIS\",\n",
    "                \"\",\n",
    "                \"### ACCOUNT INFO\",\n",
    "            ]\n",
    "\n",
    "            if user_desc:\n",
    "                lines.append(f'Bio: \"{user_desc}\"')\n",
    "\n",
    "            if user_location:\n",
    "                lines.append(f'Location: \"{user_location}\"')\n",
    "\n",
    "            # combine numeric stats into a single compact line\n",
    "            lines.append(\n",
    "                f\"Age: {age_str} | Listed: {listed_count} | Tweets: {statuses_count} | Likes: {favourites_count}\"\n",
    "            )\n",
    "\n",
    "            # append extra inferred stats if they exist\n",
    "            extra_parts = []\n",
    "            if (quoted_friends is not None) and not pd.isna(quoted_friends):\n",
    "                extra_parts.append(f\"Friends: {int(round(quoted_friends))}\")\n",
    "            if (quoted_followers is not None) and not pd.isna(quoted_followers):\n",
    "                extra_parts.append(f\"Followers: {int(round(quoted_followers))}\")\n",
    "\n",
    "            if extra_parts:\n",
    "                lines.append(\" | \".join(extra_parts))\n",
    "\n",
    "            # append extracted links if any\n",
    "            if links_str:\n",
    "                lines.append(f\"Links: {links_str}\")\n",
    "\n",
    "            lines.append(\"\")\n",
    "\n",
    "            # behavioral section showing percentage of original vs replies\n",
    "            lines.append(\"### BEHAVIOR PATTERN\")\n",
    "            lines.append(f\"Original content: {behavior_stats['pct_original']:.0f}% | Replies: {behavior_stats['pct_reply']:.0f}%\")\n",
    "            lines.append(\"\")\n",
    "\n",
    "            # sample tweets section\n",
    "            lines.append(\"### SAMPLE TWEETS\")\n",
    "\n",
    "            for i, (_, row_t) in enumerate(selected_tweets.iterrows(), 1):\n",
    "                text = row_t.get(\"tweet_text_clean\", \"\")\n",
    "\n",
    "                if isinstance(text, str) and len(text) > TWEET_MAX_LEN:\n",
    "                    text = text[:TWEET_MAX_LEN]\n",
    "\n",
    "                # add explicit prefixes to help the model understand context\n",
    "                tweet_type = row_t.get(\"tweet_type\", \"original\")\n",
    "                if tweet_type == \"reply\":\n",
    "                    type_prefix = \"[Reply] \"\n",
    "                elif tweet_type == \"quote\":\n",
    "                    type_prefix = \"[Quote] \"\n",
    "                else:\n",
    "                    type_prefix = \"[Original] \"\n",
    "\n",
    "                lines.append(f\"{i}. {type_prefix}{text}\")\n",
    "\n",
    "            # final classification question for the model\n",
    "            lines.extend([\n",
    "                \"\",\n",
    "                \"### CLASSIFICATION\",\n",
    "                \"Is this user an INFLUENCER (creates content) or OBSERVER (replies/consumes)?\",\n",
    "                \"Answer: influencer or observer\"\n",
    "            ])\n",
    "\n",
    "            # output row\n",
    "\n",
    "            row = {\n",
    "                \"user_key\": user_key,\n",
    "                \"card_id\": card_id,\n",
    "                \"prompt\": \"\\n\".join(lines),\n",
    "                \"n_tweets_total\": n_tweets,\n",
    "                \"n_tweets_in_card\": len(selected_indices),\n",
    "                \"n_cards_for_user\": n_cards,\n",
    "                \"pct_original\": behavior_stats[\"pct_original\"],\n",
    "                \"pct_reply\": behavior_stats[\"pct_reply\"],\n",
    "            }\n",
    "\n",
    "            if is_train:\n",
    "                row[\"user_label\"] = user_label\n",
    "\n",
    "            rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "# main\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    executes the full pipeline: loading, preprocessing, card generation, and saving\n",
    "    \"\"\"\n",
    "    print(\"Build user cards\")\n",
    "\n",
    "    # 1. load data\n",
    "    print(\"\\n1. Loading data\")\n",
    "\n",
    "    # load jsonl files and flatten nested structures into a dataframe\n",
    "    train_df = json_normalize(pd.read_json(TRAIN_PATH, lines=True).to_dict(orient=\"records\"))\n",
    "    test_df = json_normalize(pd.read_json(TEST_PATH, lines=True).to_dict(orient=\"records\"))\n",
    "\n",
    "    print(f\"  Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "    #  2. preprocess columns\n",
    "    print(\"\\n2. Preparing columns\")\n",
    "\n",
    "    for df_ in [train_df, test_df]:\n",
    "        # extract the most relevant text field\n",
    "        df_[\"tweet_text_raw\"] = df_.apply(get_core_tweet_text, axis=1)\n",
    "\n",
    "        # clean the text (normalize urls, strip whitespace, truncate)\n",
    "        df_[\"tweet_text_clean\"] = df_[\"tweet_text_raw\"].apply(\n",
    "            lambda x: sanitize_text(clean_tweet_urls(x), max_len=TWEET_MAX_LEN)\n",
    "        )\n",
    "\n",
    "        # parse tweet timestamps\n",
    "        df_[\"tweet_datetime\"] = pd.to_datetime(\n",
    "            df_.get(\"created_at\"),\n",
    "            format=TWITTER_DT_FORMAT,\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        # parse user creation timestamp for age calculation\n",
    "        df_[\"user_created_at_dt\"] = pd.to_datetime(\n",
    "            df_.get(\"user.created_at\"),\n",
    "            format=TWITTER_DT_FORMAT,\n",
    "            errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "    # build keys to group tweets by user\n",
    "    build_user_key(train_df)\n",
    "    build_user_key(test_df)\n",
    "\n",
    "\n",
    "    # 2bis. global map for quoted users\n",
    "\n",
    "    def build_global_quoted_mapping(train_df, test_df):\n",
    "        \"\"\"\n",
    "        constructs a global lookup table mapping quoted users to their average friend/follower counts\n",
    "        this helps recover missing metadata for users who appear as quoted content\n",
    "        \"\"\"\n",
    "        required_cols = [\n",
    "            \"quoted_status.user.created_at\",\n",
    "            \"quoted_status.user.friends_count\",\n",
    "            \"quoted_status.user.followers_count\",\n",
    "        ]\n",
    "\n",
    "        frames = []\n",
    "\n",
    "        # iterate over both train and test sets to maximize data coverage\n",
    "        for df_ in (train_df, test_df):\n",
    "\n",
    "            if not all(c in df_.columns for c in required_cols):\n",
    "                continue\n",
    "\n",
    "            # filter for rows where we actually have quoted user data\n",
    "            tmp = df_[required_cols].dropna(subset=[\"quoted_status.user.created_at\"]).copy()\n",
    "\n",
    "            if tmp.empty:\n",
    "                continue\n",
    "\n",
    "            # ensure metrics are numeric\n",
    "            tmp[\"quoted_status.user.friends_count\"] = pd.to_numeric(\n",
    "                tmp[\"quoted_status.user.friends_count\"], errors=\"coerce\"\n",
    "            )\n",
    "            tmp[\"quoted_status.user.followers_count\"] = pd.to_numeric(\n",
    "                tmp[\"quoted_status.user.followers_count\"], errors=\"coerce\"\n",
    "            )\n",
    "\n",
    "            frames.append(tmp)\n",
    "\n",
    "        if not frames:\n",
    "            return pd.DataFrame(\n",
    "                columns=[\n",
    "                    \"quoted_status.user.created_at\",\n",
    "                    \"mapped_user_friends_count\",\n",
    "                    \"mapped_user_followers_count\"\n",
    "                ]\n",
    "            ).set_index(\"quoted_status.user.created_at\")\n",
    "\n",
    "        # combine all frames and aggregate by creation date\n",
    "        all_tmp = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "        # we use mean() bc the same user might be quoted at different times\n",
    "        # with =/= counts --> averaging smoothes these variations\n",
    "        agg = all_tmp.groupby(\"quoted_status.user.created_at\").agg({\n",
    "            \"quoted_status.user.friends_count\": \"mean\",\n",
    "            \"quoted_status.user.followers_count\": \"mean\",\n",
    "        }).rename(columns={\n",
    "            \"quoted_status.user.friends_count\": \"mapped_user_friends_count\",\n",
    "            \"quoted_status.user.followers_count\": \"mapped_user_followers_count\",\n",
    "        })\n",
    "\n",
    "        return agg\n",
    "\n",
    "\n",
    "    # build the mapping table once using all available data\n",
    "    quoted_mapping = build_global_quoted_mapping(train_df, test_df)\n",
    "\n",
    "    # apply the mapping to fill in gaps\n",
    "    for df_ in (train_df, test_df):\n",
    "\n",
    "        if \"user.created_at\" in df_.columns and not quoted_mapping.empty:\n",
    "\n",
    "            # map estimated stats based on user creation date match\n",
    "            df_[\"user_friends_from_quoted\"] = df_[\"user.created_at\"].map(\n",
    "                quoted_mapping[\"mapped_user_friends_count\"]\n",
    "            )\n",
    "            df_[\"user_followers_from_quoted\"] = df_[\"user.created_at\"].map(\n",
    "                quoted_mapping[\"mapped_user_followers_count\"]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # fill with nan if mapping fails\n",
    "            df_[\"user_friends_from_quoted\"] = np.nan\n",
    "            df_[\"user_followers_from_quoted\"] = np.nan\n",
    "\n",
    "\n",
    "    # 3. generate cards\n",
    "    print(\"\\n3. Generating cards\")\n",
    "\n",
    "    train_cards = generate_user_cards(train_df, is_train=True)\n",
    "    test_cards = generate_user_cards(test_df, is_train=False)\n",
    "\n",
    "    print(f\"  Train cards: {train_cards.shape}, Test cards: {test_cards.shape}\")\n",
    "\n",
    "    # verify the split of cards per user\n",
    "    cards_per_user = train_cards.groupby(\"user_key\").size()\n",
    "\n",
    "    print(f\"\\n  Distribution: 1 card={sum(cards_per_user == 1)} users, 2 cards={sum(cards_per_user == 2)} users\")\n",
    "\n",
    "    # 4. save\n",
    "\n",
    "    train_cards.to_csv(OUT_TRAIN, index=False)\n",
    "    test_cards.to_csv(OUT_TEST, index=False)\n",
    "\n",
    "    print(f\"\\n[done] Saved: {OUT_TRAIN}\")\n",
    "    print(f\"[done] Saved: {OUT_TEST}\")\n",
    "\n",
    "    # print quick stats on the label balance\n",
    "    if \"user_label\" in train_cards.columns:\n",
    "        unique_users = train_cards.drop_duplicates(\"user_key\")\n",
    "\n",
    "        n_obs = sum(unique_users['user_label'] == 0)\n",
    "        n_inf = sum(unique_users['user_label'] == 1)\n",
    "\n",
    "        print(f\"\\nLabel distribution: Observer={n_obs}, Influencer={n_inf}\")\n",
    "\n",
    "    print(\"\\n[done] Processing complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "id": "yroElTD-pKjF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765063448954,
     "user_tz": -60,
     "elapsed": 321986,
     "user": {
      "displayName": "Avrile Floro",
      "userId": "06557886888439066437"
     }
    },
    "outputId": "2e339e04-a3e6-4f52-8f77-b3ba2ffbbc4d",
    "ExecuteTime": {
     "end_time": "2025-12-11T08:52:15.045385Z",
     "start_time": "2025-12-11T08:50:26.480309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build user cards\n",
      "\n",
      "1. Loading data\n",
      "  Train: (154914, 193), Test: (103380, 191)\n",
      "\n",
      "2. Preparing columns\n",
      "\n",
      "3. Generating cards\n",
      "  Train cards: (61337, 9), Test cards: (40894, 8)\n",
      "\n",
      "  Distribution: 1 card=55 users, 2 cards=30641 users\n",
      "\n",
      "[done] Saved: ./intermediate/user_cards_train.csv\n",
      "[done] Saved: ./intermediate/user_cards_test.csv\n",
      "\n",
      "Label distribution: Observer=16390, Influencer=14306\n",
      "\n",
      "[done] Processing complete!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```text\n",
    "Build user cards\n",
    "\n",
    "1. Loading data\n",
    "  Train: (154914, 193), Test: (103380, 191)\n",
    "\n",
    "2. Preparing columns\n",
    "\n",
    "3. Generating cards\n",
    "  Train cards: (61337, 9), Test cards: (40894, 8)\n",
    "\n",
    "  Distribution: 1 card=55 users, 2 cards=30641 users\n",
    "\n",
    "[done] Saved: ./intermediate/user_cards_train.csv\n",
    "[done] Saved: ./intermediate/user_cards_test.csv\n",
    "\n",
    "Label distribution: Observer=16390, Influencer=14306\n",
    "\n",
    "[done] Processing complete!\n",
    "```\n"
   ]
  }
 ]
}
